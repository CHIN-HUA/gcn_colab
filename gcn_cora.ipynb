{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_cora.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHIN-HUA/gcn_colab/blob/main/gcn_cora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hengqujushi/gcn_colab"
      ],
      "metadata": {
        "id": "WibmtcrJwTuu",
        "outputId": "b96bc35b-efed-40d4-bffa-9ac767a6f7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gcn_colab' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXFgbAudsyzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948b23ae-bae4-45ac-e562-0fd3f1920b9c"
      },
      "source": [
        "!python ./gcn_colab/setup.py install"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing gcn.egg-info/PKG-INFO\n",
            "writing dependency_links to gcn.egg-info/dependency_links.txt\n",
            "writing requirements to gcn.egg-info/requires.txt\n",
            "writing top-level names to gcn.egg-info/top_level.txt\n",
            "reading manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "writing manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/gcn-1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing gcn-1.0-py3.10.egg\n",
            "Removing /usr/local/lib/python3.10/dist-packages/gcn-1.0-py3.10.egg\n",
            "Copying gcn-1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "gcn 1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/gcn-1.0-py3.10.egg\n",
            "Processing dependencies for gcn==1.0\n",
            "Searching for scipy==1.10.1\n",
            "Best match: scipy 1.10.1\n",
            "Adding scipy 1.10.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.1\n",
            "Best match: networkx 3.1\n",
            "Adding networkx 3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.22.4\n",
            "Best match: numpy 1.22.4\n",
            "Adding numpy 1.22.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.10 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for gcn==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSLPHNbGs49u",
        "outputId": "8f513c8b-031a-4d10-ab5c-e7bfde4be413"
      },
      "source": [
        "%%writefile ./gcn_colab/gcn/utils.py\n",
        "## 修改utils.py\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import eigsh\n",
        "import sys\n",
        "\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def load_data(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input data from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
        "        object;\n",
        "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask,labels\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./gcn_colab/gcn/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./gcn_colab/gcn/train.py\n",
        "# 修改train.py\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask,labels = load_data(FLAGS.dataset)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "if FLAGS.model == 'gcn':\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)]  # Not used\n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.outputs], feed_dict=feed_dict)\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "# label_dict = {0:\"0\",1:\"1\",2:\"2\",3:\"3\",4:\"4\",5:\"5\",6:\"6\"} # 定义标签颜色字典\n",
        "# # 写文件\n",
        "# with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "#     for i in range(len(outs[3])):\n",
        "#         fl.write(label_dict[int(list(labels[i]).index(1.))]+\"\\n\")\n",
        "#         fe.write(\" \".join(map(str, outs[3][i]))+\"\\n\")\n",
        "label_dict = {0: \"0\", 1: \"1\"}\n",
        "# 写文件\n",
        "with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "    for i in range(len(outs[3])):\n",
        "        label_index = np.argmax(labels[i])\n",
        "        fl.write(label_dict.get(label_index, str(label_index)) + \"\\n\")\n",
        "        fe.write(\" \".join(map(str, outs[3][i])) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ],
      "metadata": {
        "id": "VqNBaMORPT-K",
        "outputId": "9de79048-502f-488d-f34d-89ca9479b9fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./gcn_colab/gcn/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./gcn_colab/gcn/layers.py\n",
        "from inits import *\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    \"\"\"Dense layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
        "                 act=tf.nn.relu, bias=False, featureless=False, save_weights=False, **kwargs):\n",
        "        super(Dense, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "        self.save_weights = save_weights\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
        "                                          name='weights')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "        if self.save_weights:\n",
        "            self.save_weights_to_file()\n",
        "\n",
        "    def save_weights_to_file(self):\n",
        "        weights = {}\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            with tf.variable_scope(self.name + '_vars', reuse=True):\n",
        "                weights['weights'] = self.vars['weights'].eval(session=sess)\n",
        "                if self.bias:\n",
        "                    weights['bias'] = self.vars['bias'].eval(session=sess)\n",
        "\n",
        "        # Save weights to file\n",
        "        filename = self.name + '_weights.txt'\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(\"GCN 权重:\\n\")\n",
        "            f.write(\"weights:\\n\")\n",
        "            np.savetxt(f, weights['weights'], fmt='%f')\n",
        "            if self.bias:\n",
        "                f.write(\"bias:\\n\")\n",
        "                np.savetxt(f, weights['bias'], fmt='%f')\n",
        "\n",
        "        print(\"已將權重寫入檔案：\", filename)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # transform\n",
        "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, save_weights=False, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.support = placeholders['support']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "        self.save_weights = save_weights\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            for i in range(len(self.support)):\n",
        "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
        "                                                        name='weights_' + str(i))\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "        if self.save_weights:\n",
        "            self.save_weights_to_file()\n",
        "\n",
        "    def save_weights_to_file(self):\n",
        "        weights = {}\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            with tf.variable_scope(self.name + '_vars', reuse=True):\n",
        "                for i in range(len(self.support)):\n",
        "                    weight_name = 'weights_' + str(i)\n",
        "                    weights[weight_name] = self.vars[weight_name].eval(session=sess)\n",
        "                if self.bias:\n",
        "                    weights['bias'] = self.vars['bias'].eval(session=sess)\n",
        "\n",
        "        # Save weights to file\n",
        "        filename = self.name + '_weights.txt'\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(\"GCN 权重:\\n\")\n",
        "            for name, weight in weights.items():\n",
        "                f.write(name + ':\\n')\n",
        "                np.savetxt(f, weight, fmt='%f')\n",
        "\n",
        "        print(\"已將權重寫入檔案：\", filename)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        supports = []\n",
        "        for i in range(len(self.support)):\n",
        "            if not self.featureless:\n",
        "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
        "                              sparse=self.sparse_inputs)\n",
        "            else:\n",
        "                pre_sup = self.vars['weights_' + str(i)]\n",
        "            support = dot(self.support[i], pre_sup, sparse=True)\n",
        "            supports.append(support)\n",
        "        output = tf.add_n(supports)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "2vnHyEK4UwSX",
        "outputId": "6fe7b372-4a54-4b6e-c30b-a73fbce8263b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./gcn_colab/gcn/layers.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./gcn_colab/gcn && python train.py "
      ],
      "metadata": {
        "id": "Zv_037xkO42s",
        "outputId": "68f136c5-44a1-4296-9ec7-c01cfd4d5049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-19 05:40:06.048790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-19 05:40:07.031359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-19 05:40:08.770235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-19 05:40:08.770367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-19 05:40:08.779551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.779668: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2023-05-19 05:40:08.789114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.789238: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2023-05-19 05:40:08.855496: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.855627: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.882306: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.882421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-19 05:40:08.956434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:08.994898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:08.995263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:08.996893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:08.997152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:08.997334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:09.903894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:09.904207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:09.904404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-19 05:40:09.904546: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-19 05:40:09.904586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-05-19 05:40:09.908261: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "Epoch: 0001 train_loss= 1.95398 train_acc= 0.14286 val_loss= 1.94916 val_acc= 0.19200 time= 0.65935\n",
            "Epoch: 0002 train_loss= 1.94717 train_acc= 0.24286 val_loss= 1.94463 val_acc= 0.25400 time= 0.01147\n",
            "Epoch: 0003 train_loss= 1.93999 train_acc= 0.40000 val_loss= 1.93989 val_acc= 0.36000 time= 0.00997\n",
            "Epoch: 0004 train_loss= 1.92993 train_acc= 0.60714 val_loss= 1.93522 val_acc= 0.38400 time= 0.00892\n",
            "Epoch: 0005 train_loss= 1.92239 train_acc= 0.62143 val_loss= 1.93089 val_acc= 0.38400 time= 0.00903\n",
            "Epoch: 0006 train_loss= 1.91587 train_acc= 0.62143 val_loss= 1.92688 val_acc= 0.38800 time= 0.00929\n",
            "Epoch: 0007 train_loss= 1.90422 train_acc= 0.65714 val_loss= 1.92295 val_acc= 0.38800 time= 0.00857\n",
            "Epoch: 0008 train_loss= 1.90035 train_acc= 0.60714 val_loss= 1.91907 val_acc= 0.37200 time= 0.00862\n",
            "Epoch: 0009 train_loss= 1.88919 train_acc= 0.61429 val_loss= 1.91513 val_acc= 0.37200 time= 0.00854\n",
            "Epoch: 0010 train_loss= 1.88187 train_acc= 0.62143 val_loss= 1.91109 val_acc= 0.36800 time= 0.00862\n",
            "Epoch: 0011 train_loss= 1.87274 train_acc= 0.64286 val_loss= 1.90697 val_acc= 0.36800 time= 0.00880\n",
            "Epoch: 0012 train_loss= 1.86119 train_acc= 0.64286 val_loss= 1.90276 val_acc= 0.37000 time= 0.01002\n",
            "Epoch: 0013 train_loss= 1.85539 train_acc= 0.61429 val_loss= 1.89846 val_acc= 0.36400 time= 0.00918\n",
            "Epoch: 0014 train_loss= 1.84110 train_acc= 0.72857 val_loss= 1.89412 val_acc= 0.36200 time= 0.00886\n",
            "Epoch: 0015 train_loss= 1.83454 train_acc= 0.62143 val_loss= 1.88983 val_acc= 0.37800 time= 0.00906\n",
            "Epoch: 0016 train_loss= 1.82084 train_acc= 0.70714 val_loss= 1.88550 val_acc= 0.39200 time= 0.00875\n",
            "Epoch: 0017 train_loss= 1.81845 train_acc= 0.64286 val_loss= 1.88113 val_acc= 0.40000 time= 0.00942\n",
            "Epoch: 0018 train_loss= 1.80250 train_acc= 0.68571 val_loss= 1.87666 val_acc= 0.40200 time= 0.00940\n",
            "Epoch: 0019 train_loss= 1.78322 train_acc= 0.72143 val_loss= 1.87212 val_acc= 0.40600 time= 0.00936\n",
            "Epoch: 0020 train_loss= 1.77089 train_acc= 0.70714 val_loss= 1.86747 val_acc= 0.40800 time= 0.00924\n",
            "Epoch: 0021 train_loss= 1.75952 train_acc= 0.74286 val_loss= 1.86282 val_acc= 0.41200 time= 0.00940\n",
            "Epoch: 0022 train_loss= 1.74195 train_acc= 0.68571 val_loss= 1.85822 val_acc= 0.41600 time= 0.01111\n",
            "Epoch: 0023 train_loss= 1.74272 train_acc= 0.68571 val_loss= 1.85360 val_acc= 0.42400 time= 0.01645\n",
            "Epoch: 0024 train_loss= 1.73830 train_acc= 0.68571 val_loss= 1.84884 val_acc= 0.43000 time= 0.00971\n",
            "Epoch: 0025 train_loss= 1.72337 train_acc= 0.70000 val_loss= 1.84389 val_acc= 0.43400 time= 0.01122\n",
            "Epoch: 0026 train_loss= 1.72588 train_acc= 0.69286 val_loss= 1.83870 val_acc= 0.44600 time= 0.00942\n",
            "Epoch: 0027 train_loss= 1.68546 train_acc= 0.77143 val_loss= 1.83333 val_acc= 0.45000 time= 0.00920\n",
            "Epoch: 0028 train_loss= 1.68721 train_acc= 0.68571 val_loss= 1.82794 val_acc= 0.46200 time= 0.00938\n",
            "Epoch: 0029 train_loss= 1.66977 train_acc= 0.75000 val_loss= 1.82235 val_acc= 0.46400 time= 0.00986\n",
            "Epoch: 0030 train_loss= 1.63515 train_acc= 0.77143 val_loss= 1.81673 val_acc= 0.47000 time= 0.00920\n",
            "Epoch: 0031 train_loss= 1.64691 train_acc= 0.76429 val_loss= 1.81108 val_acc= 0.48400 time= 0.00907\n",
            "Epoch: 0032 train_loss= 1.63115 train_acc= 0.73571 val_loss= 1.80523 val_acc= 0.49800 time= 0.01754\n",
            "Epoch: 0033 train_loss= 1.61448 train_acc= 0.76429 val_loss= 1.79926 val_acc= 0.50800 time= 0.00967\n",
            "Epoch: 0034 train_loss= 1.60044 train_acc= 0.82143 val_loss= 1.79301 val_acc= 0.51400 time= 0.00954\n",
            "Epoch: 0035 train_loss= 1.57955 train_acc= 0.80000 val_loss= 1.78670 val_acc= 0.52400 time= 0.01160\n",
            "Epoch: 0036 train_loss= 1.58841 train_acc= 0.78571 val_loss= 1.78011 val_acc= 0.53400 time= 0.00960\n",
            "Epoch: 0037 train_loss= 1.54710 train_acc= 0.80714 val_loss= 1.77347 val_acc= 0.54400 time= 0.00908\n",
            "Epoch: 0038 train_loss= 1.55767 train_acc= 0.81429 val_loss= 1.76672 val_acc= 0.55600 time= 0.00983\n",
            "Epoch: 0039 train_loss= 1.51676 train_acc= 0.82857 val_loss= 1.76003 val_acc= 0.56800 time= 0.00943\n",
            "Epoch: 0040 train_loss= 1.52725 train_acc= 0.75714 val_loss= 1.75313 val_acc= 0.58400 time= 0.00986\n",
            "Epoch: 0041 train_loss= 1.48726 train_acc= 0.87857 val_loss= 1.74616 val_acc= 0.59000 time= 0.01002\n",
            "Epoch: 0042 train_loss= 1.50069 train_acc= 0.83571 val_loss= 1.73898 val_acc= 0.59800 time= 0.00981\n",
            "Epoch: 0043 train_loss= 1.44731 train_acc= 0.83571 val_loss= 1.73153 val_acc= 0.61600 time= 0.00897\n",
            "Epoch: 0044 train_loss= 1.43205 train_acc= 0.84286 val_loss= 1.72379 val_acc= 0.64400 time= 0.00855\n",
            "Epoch: 0045 train_loss= 1.46252 train_acc= 0.85714 val_loss= 1.71587 val_acc= 0.65600 time= 0.00904\n",
            "Epoch: 0046 train_loss= 1.44155 train_acc= 0.85000 val_loss= 1.70774 val_acc= 0.67000 time= 0.00919\n",
            "Epoch: 0047 train_loss= 1.40873 train_acc= 0.86429 val_loss= 1.69973 val_acc= 0.68000 time= 0.00881\n",
            "Epoch: 0048 train_loss= 1.39569 train_acc= 0.85714 val_loss= 1.69173 val_acc= 0.68200 time= 0.00904\n",
            "Epoch: 0049 train_loss= 1.38352 train_acc= 0.88571 val_loss= 1.68383 val_acc= 0.68800 time= 0.00876\n",
            "Epoch: 0050 train_loss= 1.37835 train_acc= 0.85000 val_loss= 1.67588 val_acc= 0.68800 time= 0.00943\n",
            "Epoch: 0051 train_loss= 1.34691 train_acc= 0.87143 val_loss= 1.66812 val_acc= 0.69600 time= 0.00926\n",
            "Epoch: 0052 train_loss= 1.31211 train_acc= 0.91429 val_loss= 1.66018 val_acc= 0.70400 time= 0.00910\n",
            "Epoch: 0053 train_loss= 1.33207 train_acc= 0.90000 val_loss= 1.65215 val_acc= 0.71400 time= 0.01006\n",
            "Epoch: 0054 train_loss= 1.26994 train_acc= 0.88571 val_loss= 1.64410 val_acc= 0.71600 time= 0.01078\n",
            "Epoch: 0055 train_loss= 1.30618 train_acc= 0.89286 val_loss= 1.63633 val_acc= 0.71800 time= 0.00900\n",
            "Epoch: 0056 train_loss= 1.26758 train_acc= 0.90714 val_loss= 1.62807 val_acc= 0.72200 time= 0.00895\n",
            "Epoch: 0057 train_loss= 1.24267 train_acc= 0.89286 val_loss= 1.61949 val_acc= 0.72400 time= 0.00898\n",
            "Epoch: 0058 train_loss= 1.25285 train_acc= 0.89286 val_loss= 1.61089 val_acc= 0.73000 time= 0.00933\n",
            "Epoch: 0059 train_loss= 1.27309 train_acc= 0.91429 val_loss= 1.60223 val_acc= 0.73800 time= 0.00926\n",
            "Epoch: 0060 train_loss= 1.23682 train_acc= 0.91429 val_loss= 1.59340 val_acc= 0.73600 time= 0.00939\n",
            "Epoch: 0061 train_loss= 1.19454 train_acc= 0.91429 val_loss= 1.58448 val_acc= 0.73600 time= 0.00919\n",
            "Epoch: 0062 train_loss= 1.17074 train_acc= 0.92143 val_loss= 1.57549 val_acc= 0.73800 time= 0.00946\n",
            "Epoch: 0063 train_loss= 1.16221 train_acc= 0.93571 val_loss= 1.56648 val_acc= 0.74200 time= 0.01448\n",
            "Epoch: 0064 train_loss= 1.15121 train_acc= 0.92143 val_loss= 1.55735 val_acc= 0.74800 time= 0.00909\n",
            "Epoch: 0065 train_loss= 1.16159 train_acc= 0.88571 val_loss= 1.54846 val_acc= 0.75400 time= 0.00892\n",
            "Epoch: 0066 train_loss= 1.18634 train_acc= 0.91429 val_loss= 1.53992 val_acc= 0.75400 time= 0.00895\n",
            "Epoch: 0067 train_loss= 1.12456 train_acc= 0.90000 val_loss= 1.53127 val_acc= 0.75600 time= 0.00886\n",
            "Epoch: 0068 train_loss= 1.12594 train_acc= 0.93571 val_loss= 1.52269 val_acc= 0.76000 time= 0.00851\n",
            "Epoch: 0069 train_loss= 1.12804 train_acc= 0.87143 val_loss= 1.51446 val_acc= 0.76000 time= 0.00879\n",
            "Epoch: 0070 train_loss= 1.07804 train_acc= 0.92857 val_loss= 1.50658 val_acc= 0.76000 time= 0.00890\n",
            "Epoch: 0071 train_loss= 1.05328 train_acc= 0.95000 val_loss= 1.49881 val_acc= 0.76200 time= 0.00853\n",
            "Epoch: 0072 train_loss= 1.09813 train_acc= 0.93571 val_loss= 1.49116 val_acc= 0.76400 time= 0.00883\n",
            "Epoch: 0073 train_loss= 1.02339 train_acc= 0.94286 val_loss= 1.48338 val_acc= 0.76000 time= 0.00883\n",
            "Epoch: 0074 train_loss= 1.02378 train_acc= 0.95714 val_loss= 1.47552 val_acc= 0.75800 time= 0.00891\n",
            "Epoch: 0075 train_loss= 1.01290 train_acc= 0.95000 val_loss= 1.46789 val_acc= 0.76000 time= 0.00900\n",
            "Epoch: 0076 train_loss= 1.06556 train_acc= 0.89286 val_loss= 1.46047 val_acc= 0.75800 time= 0.01312\n",
            "Epoch: 0077 train_loss= 1.01221 train_acc= 0.91429 val_loss= 1.45304 val_acc= 0.76200 time= 0.00894\n",
            "Epoch: 0078 train_loss= 1.00247 train_acc= 0.95000 val_loss= 1.44549 val_acc= 0.76200 time= 0.00966\n",
            "Epoch: 0079 train_loss= 1.04554 train_acc= 0.92857 val_loss= 1.43810 val_acc= 0.76200 time= 0.00890\n",
            "Epoch: 0080 train_loss= 1.04430 train_acc= 0.91429 val_loss= 1.43045 val_acc= 0.76200 time= 0.00899\n",
            "Epoch: 0081 train_loss= 0.97607 train_acc= 0.97143 val_loss= 1.42323 val_acc= 0.76400 time= 0.00898\n",
            "Epoch: 0082 train_loss= 0.99655 train_acc= 0.92143 val_loss= 1.41607 val_acc= 0.76600 time= 0.00865\n",
            "Epoch: 0083 train_loss= 0.97819 train_acc= 0.91429 val_loss= 1.40884 val_acc= 0.77200 time= 0.00921\n",
            "Epoch: 0084 train_loss= 0.96876 train_acc= 0.95000 val_loss= 1.40193 val_acc= 0.77600 time= 0.01244\n",
            "Epoch: 0085 train_loss= 0.93855 train_acc= 0.94286 val_loss= 1.39511 val_acc= 0.78200 time= 0.00927\n",
            "Epoch: 0086 train_loss= 0.96155 train_acc= 0.92857 val_loss= 1.38860 val_acc= 0.78200 time= 0.00905\n",
            "Epoch: 0087 train_loss= 0.96553 train_acc= 0.95714 val_loss= 1.38206 val_acc= 0.78200 time= 0.01160\n",
            "Epoch: 0088 train_loss= 0.93760 train_acc= 0.92143 val_loss= 1.37563 val_acc= 0.78200 time= 0.01508\n",
            "Epoch: 0089 train_loss= 0.91658 train_acc= 0.94286 val_loss= 1.36967 val_acc= 0.78200 time= 0.01200\n",
            "Epoch: 0090 train_loss= 0.94048 train_acc= 0.94286 val_loss= 1.36404 val_acc= 0.78200 time= 0.00963\n",
            "Epoch: 0091 train_loss= 0.89515 train_acc= 0.95714 val_loss= 1.35863 val_acc= 0.78200 time= 0.00950\n",
            "Epoch: 0092 train_loss= 0.93473 train_acc= 0.94286 val_loss= 1.35296 val_acc= 0.78400 time= 0.00872\n",
            "Epoch: 0093 train_loss= 0.90961 train_acc= 0.94286 val_loss= 1.34768 val_acc= 0.78200 time= 0.00969\n",
            "Epoch: 0094 train_loss= 0.91874 train_acc= 0.91429 val_loss= 1.34313 val_acc= 0.78200 time= 0.00892\n",
            "Epoch: 0095 train_loss= 0.93025 train_acc= 0.91429 val_loss= 1.33903 val_acc= 0.78200 time= 0.00869\n",
            "Epoch: 0096 train_loss= 0.92147 train_acc= 0.95000 val_loss= 1.33505 val_acc= 0.78400 time= 0.00854\n",
            "Epoch: 0097 train_loss= 0.87581 train_acc= 0.93571 val_loss= 1.33095 val_acc= 0.78400 time= 0.00848\n",
            "Epoch: 0098 train_loss= 0.89958 train_acc= 0.96429 val_loss= 1.32702 val_acc= 0.78600 time= 0.00906\n",
            "Epoch: 0099 train_loss= 0.89164 train_acc= 0.95000 val_loss= 1.32282 val_acc= 0.78600 time= 0.00893\n",
            "Epoch: 0100 train_loss= 0.83943 train_acc= 0.97143 val_loss= 1.31836 val_acc= 0.78400 time= 0.00851\n",
            "Epoch: 0101 train_loss= 0.89123 train_acc= 0.92857 val_loss= 1.31348 val_acc= 0.78000 time= 0.01490\n",
            "Epoch: 0102 train_loss= 0.84613 train_acc= 0.92857 val_loss= 1.30786 val_acc= 0.78200 time= 0.00916\n",
            "Epoch: 0103 train_loss= 0.85106 train_acc= 0.94286 val_loss= 1.30189 val_acc= 0.78200 time= 0.00909\n",
            "Epoch: 0104 train_loss= 0.83774 train_acc= 0.92857 val_loss= 1.29585 val_acc= 0.78600 time= 0.00933\n",
            "Epoch: 0105 train_loss= 0.82232 train_acc= 0.96429 val_loss= 1.28971 val_acc= 0.78600 time= 0.00924\n",
            "Epoch: 0106 train_loss= 0.84253 train_acc= 0.95714 val_loss= 1.28336 val_acc= 0.78600 time= 0.00889\n",
            "Epoch: 0107 train_loss= 0.83385 train_acc= 0.95000 val_loss= 1.27727 val_acc= 0.78600 time= 0.00893\n",
            "Epoch: 0108 train_loss= 0.83776 train_acc= 0.96429 val_loss= 1.27111 val_acc= 0.78600 time= 0.00850\n",
            "Epoch: 0109 train_loss= 0.83914 train_acc= 0.95000 val_loss= 1.26563 val_acc= 0.79000 time= 0.00908\n",
            "Epoch: 0110 train_loss= 0.81960 train_acc= 0.95000 val_loss= 1.26053 val_acc= 0.79000 time= 0.00831\n",
            "Epoch: 0111 train_loss= 0.80221 train_acc= 0.94286 val_loss= 1.25579 val_acc= 0.79000 time= 0.00883\n",
            "Epoch: 0112 train_loss= 0.84440 train_acc= 0.93571 val_loss= 1.25109 val_acc= 0.79000 time= 0.00848\n",
            "Epoch: 0113 train_loss= 0.82257 train_acc= 0.95714 val_loss= 1.24688 val_acc= 0.79000 time= 0.00832\n",
            "Epoch: 0114 train_loss= 0.81499 train_acc= 0.94286 val_loss= 1.24287 val_acc= 0.79000 time= 0.00857\n",
            "Epoch: 0115 train_loss= 0.83107 train_acc= 0.94286 val_loss= 1.23924 val_acc= 0.79000 time= 0.01311\n",
            "Epoch: 0116 train_loss= 0.80484 train_acc= 0.92857 val_loss= 1.23566 val_acc= 0.78800 time= 0.00905\n",
            "Epoch: 0117 train_loss= 0.78614 train_acc= 0.95714 val_loss= 1.23240 val_acc= 0.78600 time= 0.00862\n",
            "Epoch: 0118 train_loss= 0.77831 train_acc= 0.94286 val_loss= 1.22966 val_acc= 0.78600 time= 0.00859\n",
            "Epoch: 0119 train_loss= 0.77073 train_acc= 0.95000 val_loss= 1.22724 val_acc= 0.78800 time= 0.00877\n",
            "Epoch: 0120 train_loss= 0.77642 train_acc= 0.96429 val_loss= 1.22486 val_acc= 0.78800 time= 0.00895\n",
            "Epoch: 0121 train_loss= 0.76526 train_acc= 0.93571 val_loss= 1.22220 val_acc= 0.78800 time= 0.00919\n",
            "Epoch: 0122 train_loss= 0.75809 train_acc= 0.96429 val_loss= 1.21980 val_acc= 0.78800 time= 0.00880\n",
            "Epoch: 0123 train_loss= 0.74995 train_acc= 0.97143 val_loss= 1.21724 val_acc= 0.78800 time= 0.00859\n",
            "Epoch: 0124 train_loss= 0.77459 train_acc= 0.93571 val_loss= 1.21434 val_acc= 0.79000 time= 0.00907\n",
            "Epoch: 0125 train_loss= 0.77623 train_acc= 0.97143 val_loss= 1.21180 val_acc= 0.79000 time= 0.00867\n",
            "Epoch: 0126 train_loss= 0.72542 train_acc= 0.95714 val_loss= 1.20932 val_acc= 0.79200 time= 0.00971\n",
            "Epoch: 0127 train_loss= 0.74450 train_acc= 0.93571 val_loss= 1.20658 val_acc= 0.79200 time= 0.00941\n",
            "Epoch: 0128 train_loss= 0.73371 train_acc= 0.97857 val_loss= 1.20363 val_acc= 0.79200 time= 0.00910\n",
            "Epoch: 0129 train_loss= 0.71551 train_acc= 0.96429 val_loss= 1.20069 val_acc= 0.79200 time= 0.00873\n",
            "Epoch: 0130 train_loss= 0.77533 train_acc= 0.94286 val_loss= 1.19739 val_acc= 0.79200 time= 0.00944\n",
            "Epoch: 0131 train_loss= 0.72124 train_acc= 0.96429 val_loss= 1.19431 val_acc= 0.79200 time= 0.00961\n",
            "Epoch: 0132 train_loss= 0.78295 train_acc= 0.95000 val_loss= 1.19175 val_acc= 0.79200 time= 0.00945\n",
            "Epoch: 0133 train_loss= 0.71783 train_acc= 0.95000 val_loss= 1.18917 val_acc= 0.79000 time= 0.00899\n",
            "Epoch: 0134 train_loss= 0.76042 train_acc= 0.95000 val_loss= 1.18628 val_acc= 0.79200 time= 0.00978\n",
            "Epoch: 0135 train_loss= 0.69706 train_acc= 0.97143 val_loss= 1.18309 val_acc= 0.79000 time= 0.00938\n",
            "Epoch: 0136 train_loss= 0.74125 train_acc= 0.95714 val_loss= 1.18018 val_acc= 0.79000 time= 0.00982\n",
            "Epoch: 0137 train_loss= 0.74695 train_acc= 0.95714 val_loss= 1.17701 val_acc= 0.79000 time= 0.01412\n",
            "Epoch: 0138 train_loss= 0.70791 train_acc= 0.94286 val_loss= 1.17432 val_acc= 0.78800 time= 0.00911\n",
            "Epoch: 0139 train_loss= 0.73143 train_acc= 0.94286 val_loss= 1.17187 val_acc= 0.79000 time= 0.00960\n",
            "Epoch: 0140 train_loss= 0.69934 train_acc= 0.97143 val_loss= 1.16926 val_acc= 0.78800 time= 0.01031\n",
            "Epoch: 0141 train_loss= 0.71071 train_acc= 0.94286 val_loss= 1.16684 val_acc= 0.78800 time= 0.00896\n",
            "Epoch: 0142 train_loss= 0.67117 train_acc= 0.96429 val_loss= 1.16472 val_acc= 0.78800 time= 0.00899\n",
            "Epoch: 0143 train_loss= 0.67728 train_acc= 0.97143 val_loss= 1.16291 val_acc= 0.78800 time= 0.00869\n",
            "Epoch: 0144 train_loss= 0.72865 train_acc= 0.95714 val_loss= 1.16087 val_acc= 0.78800 time= 0.00876\n",
            "Epoch: 0145 train_loss= 0.70414 train_acc= 0.95714 val_loss= 1.15891 val_acc= 0.78400 time= 0.00952\n",
            "Epoch: 0146 train_loss= 0.71027 train_acc= 0.95000 val_loss= 1.15753 val_acc= 0.78400 time= 0.00888\n",
            "Epoch: 0147 train_loss= 0.70071 train_acc= 0.97857 val_loss= 1.15546 val_acc= 0.78600 time= 0.00936\n",
            "Epoch: 0148 train_loss= 0.70648 train_acc= 0.94286 val_loss= 1.15337 val_acc= 0.78600 time= 0.00868\n",
            "Epoch: 0149 train_loss= 0.66662 train_acc= 0.97143 val_loss= 1.15110 val_acc= 0.78600 time= 0.00907\n",
            "Epoch: 0150 train_loss= 0.68066 train_acc= 0.99286 val_loss= 1.14923 val_acc= 0.78600 time= 0.00946\n",
            "Epoch: 0151 train_loss= 0.70275 train_acc= 0.96429 val_loss= 1.14772 val_acc= 0.78600 time= 0.00937\n",
            "Epoch: 0152 train_loss= 0.70096 train_acc= 0.95714 val_loss= 1.14628 val_acc= 0.78600 time= 0.01076\n",
            "Epoch: 0153 train_loss= 0.68893 train_acc= 0.95714 val_loss= 1.14501 val_acc= 0.78600 time= 0.00903\n",
            "Epoch: 0154 train_loss= 0.68354 train_acc= 0.96429 val_loss= 1.14304 val_acc= 0.78600 time= 0.01048\n",
            "Epoch: 0155 train_loss= 0.67374 train_acc= 0.97143 val_loss= 1.14123 val_acc= 0.78600 time= 0.00893\n",
            "Epoch: 0156 train_loss= 0.63494 train_acc= 0.99286 val_loss= 1.13956 val_acc= 0.78800 time= 0.00911\n",
            "Epoch: 0157 train_loss= 0.65656 train_acc= 0.96429 val_loss= 1.13783 val_acc= 0.79000 time= 0.00938\n",
            "Epoch: 0158 train_loss= 0.65648 train_acc= 0.95714 val_loss= 1.13580 val_acc= 0.79200 time= 0.00922\n",
            "Epoch: 0159 train_loss= 0.71239 train_acc= 0.93571 val_loss= 1.13380 val_acc= 0.79200 time= 0.00922\n",
            "Epoch: 0160 train_loss= 0.68017 train_acc= 0.94286 val_loss= 1.13143 val_acc= 0.79200 time= 0.00900\n",
            "Epoch: 0161 train_loss= 0.67913 train_acc= 0.95714 val_loss= 1.12902 val_acc= 0.79200 time= 0.00884\n",
            "Epoch: 0162 train_loss= 0.68679 train_acc= 0.92857 val_loss= 1.12701 val_acc= 0.79000 time= 0.00885\n",
            "Epoch: 0163 train_loss= 0.68635 train_acc= 0.95714 val_loss= 1.12556 val_acc= 0.79000 time= 0.00893\n",
            "Epoch: 0164 train_loss= 0.63512 train_acc= 0.98571 val_loss= 1.12359 val_acc= 0.78800 time= 0.00928\n",
            "Epoch: 0165 train_loss= 0.67694 train_acc= 0.95000 val_loss= 1.12229 val_acc= 0.78800 time= 0.00947\n",
            "Epoch: 0166 train_loss= 0.67551 train_acc= 0.96429 val_loss= 1.12105 val_acc= 0.78400 time= 0.00899\n",
            "Epoch: 0167 train_loss= 0.69947 train_acc= 0.95000 val_loss= 1.11963 val_acc= 0.78400 time= 0.00894\n",
            "Epoch: 0168 train_loss= 0.66728 train_acc= 0.97143 val_loss= 1.11815 val_acc= 0.78400 time= 0.01630\n",
            "Epoch: 0169 train_loss= 0.66971 train_acc= 0.97143 val_loss= 1.11681 val_acc= 0.78400 time= 0.00910\n",
            "Epoch: 0170 train_loss= 0.62243 train_acc= 0.98571 val_loss= 1.11476 val_acc= 0.78400 time= 0.00940\n",
            "Epoch: 0171 train_loss= 0.66034 train_acc= 0.96429 val_loss= 1.11329 val_acc= 0.78400 time= 0.00905\n",
            "Epoch: 0172 train_loss= 0.70384 train_acc= 0.95000 val_loss= 1.11188 val_acc= 0.78400 time= 0.02294\n",
            "Epoch: 0173 train_loss= 0.63970 train_acc= 0.97143 val_loss= 1.10982 val_acc= 0.78400 time= 0.00915\n",
            "Epoch: 0174 train_loss= 0.68714 train_acc= 0.95714 val_loss= 1.10740 val_acc= 0.78800 time= 0.00956\n",
            "Epoch: 0175 train_loss= 0.68592 train_acc= 0.96429 val_loss= 1.10473 val_acc= 0.78800 time= 0.00910\n",
            "Epoch: 0176 train_loss= 0.64401 train_acc= 0.97143 val_loss= 1.10238 val_acc= 0.78800 time= 0.00925\n",
            "Epoch: 0177 train_loss= 0.65554 train_acc= 0.96429 val_loss= 1.10005 val_acc= 0.79200 time= 0.00906\n",
            "Epoch: 0178 train_loss= 0.64684 train_acc= 0.94286 val_loss= 1.09770 val_acc= 0.79200 time= 0.00868\n",
            "Epoch: 0179 train_loss= 0.62039 train_acc= 0.94286 val_loss= 1.09502 val_acc= 0.79200 time= 0.00883\n",
            "Epoch: 0180 train_loss= 0.64569 train_acc= 0.93571 val_loss= 1.09291 val_acc= 0.79200 time= 0.00882\n",
            "Epoch: 0181 train_loss= 0.62059 train_acc= 0.95000 val_loss= 1.09067 val_acc= 0.79200 time= 0.00852\n",
            "Epoch: 0182 train_loss= 0.62461 train_acc= 0.96429 val_loss= 1.08900 val_acc= 0.79000 time= 0.00893\n",
            "Epoch: 0183 train_loss= 0.62334 train_acc= 0.96429 val_loss= 1.08723 val_acc= 0.79000 time= 0.00869\n",
            "Epoch: 0184 train_loss= 0.65683 train_acc= 0.95000 val_loss= 1.08566 val_acc= 0.79000 time= 0.00838\n",
            "Epoch: 0185 train_loss= 0.66517 train_acc= 0.96429 val_loss= 1.08404 val_acc= 0.79000 time= 0.00876\n",
            "Epoch: 0186 train_loss= 0.68030 train_acc= 0.94286 val_loss= 1.08288 val_acc= 0.79000 time= 0.00887\n",
            "Epoch: 0187 train_loss= 0.60279 train_acc= 0.95000 val_loss= 1.08196 val_acc= 0.79000 time= 0.01646\n",
            "Epoch: 0188 train_loss= 0.60231 train_acc= 0.97857 val_loss= 1.08095 val_acc= 0.79000 time= 0.01337\n",
            "Epoch: 0189 train_loss= 0.61390 train_acc= 0.96429 val_loss= 1.07952 val_acc= 0.79000 time= 0.01260\n",
            "Epoch: 0190 train_loss= 0.59581 train_acc= 0.96429 val_loss= 1.07784 val_acc= 0.79000 time= 0.01457\n",
            "Epoch: 0191 train_loss= 0.60097 train_acc= 0.97857 val_loss= 1.07597 val_acc= 0.79200 time= 0.01394\n",
            "Epoch: 0192 train_loss= 0.61073 train_acc= 0.90714 val_loss= 1.07484 val_acc= 0.79400 time= 0.01536\n",
            "Epoch: 0193 train_loss= 0.61191 train_acc= 0.95714 val_loss= 1.07386 val_acc= 0.79600 time= 0.01327\n",
            "Epoch: 0194 train_loss= 0.63342 train_acc= 0.95714 val_loss= 1.07295 val_acc= 0.79600 time= 0.01352\n",
            "Epoch: 0195 train_loss= 0.67858 train_acc= 0.92857 val_loss= 1.07154 val_acc= 0.79600 time= 0.01244\n",
            "Epoch: 0196 train_loss= 0.60298 train_acc= 0.97857 val_loss= 1.06956 val_acc= 0.79400 time= 0.01360\n",
            "Epoch: 0197 train_loss= 0.64595 train_acc= 0.95714 val_loss= 1.06777 val_acc= 0.79200 time= 0.01340\n",
            "Epoch: 0198 train_loss= 0.62150 train_acc= 0.97143 val_loss= 1.06582 val_acc= 0.79400 time= 0.01321\n",
            "Epoch: 0199 train_loss= 0.62767 train_acc= 0.94286 val_loss= 1.06380 val_acc= 0.79400 time= 0.01552\n",
            "Epoch: 0200 train_loss= 0.60381 train_acc= 0.93571 val_loss= 1.06281 val_acc= 0.79400 time= 0.01372\n",
            "Optimization Finished!\n",
            "Test set results: cost= 1.01652 accuracy= 0.81800 time= 0.00650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "x, y = [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_x = pca.fit_transform(x)\n",
        "\n",
        "color = ['blue', 'red']\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def update(frame):\n",
        "    ax.cla()\n",
        "    for index, item in enumerate(reduced_x):\n",
        "        if index < len(y):\n",
        "            label_index = min(y[index], len(color) - 1)\n",
        "            ax.scatter(item[0], item[1], c=color[label_index])\n",
        "    ax.set_title(\"GCN Visualization\")\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=100, interval=200)\n",
        "HTML(ani.to_jshtml())\n",
        "plt.show()\n",
        "\n",
        "# # 保存動畫\n",
        "# ani.save(\"gcn_animation.mp4\", writer=\"ffmpeg\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8DiVYdLh8rZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "ANkS0HSU1SVl",
        "outputId": "0dc971f8-7abe-4871-e8ee-e4aa8a0f90a6"
      },
      "source": [
        "import matplotlib.pyplot as plt                 #加载matplotlib用于数据的可视化\n",
        "from sklearn.decomposition import PCA           #加载PCA算法包\n",
        "\n",
        "x, y= [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:  # 打开文件\n",
        "    data1 = f.read().strip().split(\"\\n\")  # 读取文件\n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:  # 打开文件\n",
        "    data1 = f.read().strip().split(\"\\n\")  # 读取文件\n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca=PCA(n_components=2)     #加载PCA算法，设置降维后主成分数目为2\n",
        "reduced_x=pca.fit_transform(x)#对样本进行降维\n",
        "print(reduced_x)\n",
        "\n",
        "# #可视化\n",
        "color = ['blue', 'red']\n",
        "for index, item in enumerate(reduced_x):\n",
        "    label_index = min(y[index], len(color) - 1)\n",
        "    plt.scatter(item[0], item[1], c=color[label_index])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.23694408  2.30129365]\n",
            " [ 0.16284254  1.35878366]\n",
            " [-2.00710953 -1.71590138]\n",
            " ...\n",
            " [ 0.52557286  0.21814753]\n",
            " [-1.47912348 -1.81866282]\n",
            " [ 1.07136497  0.27522435]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEm1k2JOwPPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuqWccnq3XO",
        "outputId": "6e5da97c-ce25-4cf2-8e51-46a7019bfa24"
      },
      "source": [
        "\n",
        "\n",
        "# 修改train.py\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask,labels = load_data(FLAGS.dataset)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "if FLAGS.model == 'gcn':\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)]  # Not used\n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.outputs], feed_dict=feed_dict)\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "# label_dict = {0:\"0\",1:\"1\",2:\"2\",3:\"3\",4:\"4\",5:\"5\",6:\"6\"} # 定义标签颜色字典\n",
        "# # 写文件\n",
        "# with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "#     for i in range(len(outs[3])):\n",
        "#         fl.write(label_dict[int(list(labels[i]).index(1.))]+\"\\n\")\n",
        "#         fe.write(\" \".join(map(str, outs[3][i]))+\"\\n\")\n",
        "label_dict = {0: \"0\", 1: \"1\"}\n",
        "# 写文件\n",
        "with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "    for i in range(len(outs[3])):\n",
        "        label_index = np.argmax(labels[i])\n",
        "        fl.write(label_dict.get(label_index, str(label_index)) + \"\\n\")\n",
        "        fe.write(\" \".join(map(str, outs[3][i])) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gcn_colab'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 60 (delta 11), reused 57 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), 5.25 MiB | 1.73 MiB/s, done.\n"
          ]
        }
      ]
    }
  ]
}