{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_cora.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHIN-HUA/gcn_colab/blob/main/gcn_cora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuqWccnq3XO",
        "outputId": "6e5da97c-ce25-4cf2-8e51-46a7019bfa24"
      },
      "source": [
        "!git clone https://github.com/hengqujushi/gcn_colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gcn_colab'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 60 (delta 11), reused 57 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), 5.25 MiB | 1.73 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXFgbAudsyzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8be435-1a3c-48b9-8f0a-fb34813d6537"
      },
      "source": [
        "!python ./gcn_colab/setup.py install"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating gcn.egg-info\n",
            "writing gcn.egg-info/PKG-INFO\n",
            "writing dependency_links to gcn.egg-info/dependency_links.txt\n",
            "writing requirements to gcn.egg-info/requires.txt\n",
            "writing top-level names to gcn.egg-info/top_level.txt\n",
            "writing manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "reading manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "writing manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/gcn-1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing gcn-1.0-py3.10.egg\n",
            "Copying gcn-1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding gcn 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/gcn-1.0-py3.10.egg\n",
            "Processing dependencies for gcn==1.0\n",
            "Searching for scipy==1.10.1\n",
            "Best match: scipy 1.10.1\n",
            "Adding scipy 1.10.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.1\n",
            "Best match: networkx 3.1\n",
            "Adding networkx 3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.22.4\n",
            "Best match: numpy 1.22.4\n",
            "Adding numpy 1.22.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.10 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for gcn==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSLPHNbGs49u",
        "outputId": "0aa3d521-b5fd-4dcb-d32a-efebec9e0784"
      },
      "source": [
        "\n",
        "!cd ./gcn_colab/gcn && python train.py --dataset citeseer\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-16 05:04:47.806690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-16 05:04:49.309722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/gcn_colab/gcn/utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n",
            "2023-05-16 05:04:51.953129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-16 05:04:51.953276: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-16 05:04:51.963541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:51.963673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2023-05-16 05:04:51.973583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:51.973709: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2023-05-16 05:04:52.047385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:52.047546: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:52.079114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:52.079283: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 05:04:52.161255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:52.191881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:52.192161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:52.192974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:52.193215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:52.193392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:53.152067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:53.152344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:53.152543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 05:04:53.152678: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-16 05:04:53.152742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-05-16 05:04:53.156680: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "Epoch: 0001 train_loss= 1.79979 train_acc= 0.17500 val_loss= 1.79528 val_acc= 0.41400 time= 0.72153\n",
            "Epoch: 0002 train_loss= 1.79284 train_acc= 0.50000 val_loss= 1.79141 val_acc= 0.48200 time= 0.01701\n",
            "Epoch: 0003 train_loss= 1.78730 train_acc= 0.56667 val_loss= 1.78799 val_acc= 0.45000 time= 0.01779\n",
            "Epoch: 0004 train_loss= 1.78071 train_acc= 0.63333 val_loss= 1.78535 val_acc= 0.44400 time= 0.01611\n",
            "Epoch: 0005 train_loss= 1.77303 train_acc= 0.70833 val_loss= 1.78328 val_acc= 0.42000 time= 0.01562\n",
            "Epoch: 0006 train_loss= 1.76666 train_acc= 0.72500 val_loss= 1.78149 val_acc= 0.40800 time= 0.01570\n",
            "Epoch: 0007 train_loss= 1.76043 train_acc= 0.70000 val_loss= 1.77979 val_acc= 0.39600 time= 0.01589\n",
            "Epoch: 0008 train_loss= 1.75599 train_acc= 0.66667 val_loss= 1.77814 val_acc= 0.40000 time= 0.01610\n",
            "Epoch: 0009 train_loss= 1.74610 train_acc= 0.75833 val_loss= 1.77649 val_acc= 0.41400 time= 0.01571\n",
            "Epoch: 0010 train_loss= 1.73455 train_acc= 0.76667 val_loss= 1.77490 val_acc= 0.42800 time= 0.01611\n",
            "Epoch: 0011 train_loss= 1.73058 train_acc= 0.75000 val_loss= 1.77340 val_acc= 0.42800 time= 0.01655\n",
            "Epoch: 0012 train_loss= 1.72558 train_acc= 0.70000 val_loss= 1.77197 val_acc= 0.43200 time= 0.01628\n",
            "Epoch: 0013 train_loss= 1.71449 train_acc= 0.81667 val_loss= 1.77054 val_acc= 0.43000 time= 0.01609\n",
            "Epoch: 0014 train_loss= 1.71075 train_acc= 0.78333 val_loss= 1.76908 val_acc= 0.42800 time= 0.02696\n",
            "Epoch: 0015 train_loss= 1.70172 train_acc= 0.79167 val_loss= 1.76769 val_acc= 0.42600 time= 0.02162\n",
            "Epoch: 0016 train_loss= 1.69308 train_acc= 0.75833 val_loss= 1.76629 val_acc= 0.42800 time= 0.01685\n",
            "Epoch: 0017 train_loss= 1.68974 train_acc= 0.77500 val_loss= 1.76488 val_acc= 0.42800 time= 0.01582\n",
            "Epoch: 0018 train_loss= 1.67082 train_acc= 0.83333 val_loss= 1.76345 val_acc= 0.43000 time= 0.01582\n",
            "Epoch: 0019 train_loss= 1.67379 train_acc= 0.75833 val_loss= 1.76186 val_acc= 0.43200 time= 0.01598\n",
            "Epoch: 0020 train_loss= 1.65387 train_acc= 0.78333 val_loss= 1.76031 val_acc= 0.44200 time= 0.01560\n",
            "Epoch: 0021 train_loss= 1.64359 train_acc= 0.80833 val_loss= 1.75878 val_acc= 0.44000 time= 0.01595\n",
            "Epoch: 0022 train_loss= 1.63355 train_acc= 0.82500 val_loss= 1.75718 val_acc= 0.44200 time= 0.01563\n",
            "Epoch: 0023 train_loss= 1.64585 train_acc= 0.80833 val_loss= 1.75553 val_acc= 0.44400 time= 0.01592\n",
            "Epoch: 0024 train_loss= 1.63481 train_acc= 0.75833 val_loss= 1.75381 val_acc= 0.45200 time= 0.01573\n",
            "Epoch: 0025 train_loss= 1.62309 train_acc= 0.76667 val_loss= 1.75199 val_acc= 0.46400 time= 0.01549\n",
            "Epoch: 0026 train_loss= 1.59889 train_acc= 0.76667 val_loss= 1.75010 val_acc= 0.47200 time= 0.01770\n",
            "Epoch: 0027 train_loss= 1.59010 train_acc= 0.82500 val_loss= 1.74817 val_acc= 0.48400 time= 0.01545\n",
            "Epoch: 0028 train_loss= 1.58626 train_acc= 0.80000 val_loss= 1.74611 val_acc= 0.50400 time= 0.01594\n",
            "Epoch: 0029 train_loss= 1.56705 train_acc= 0.84167 val_loss= 1.74399 val_acc= 0.51400 time= 0.01532\n",
            "Epoch: 0030 train_loss= 1.55928 train_acc= 0.83333 val_loss= 1.74174 val_acc= 0.54200 time= 0.01542\n",
            "Epoch: 0031 train_loss= 1.55362 train_acc= 0.81667 val_loss= 1.73942 val_acc= 0.54200 time= 0.01537\n",
            "Epoch: 0032 train_loss= 1.53945 train_acc= 0.85000 val_loss= 1.73704 val_acc= 0.55000 time= 0.01622\n",
            "Epoch: 0033 train_loss= 1.54849 train_acc= 0.82500 val_loss= 1.73461 val_acc= 0.56400 time= 0.01617\n",
            "Epoch: 0034 train_loss= 1.54541 train_acc= 0.83333 val_loss= 1.73214 val_acc= 0.56800 time= 0.02054\n",
            "Epoch: 0035 train_loss= 1.52626 train_acc= 0.80833 val_loss= 1.72962 val_acc= 0.57800 time= 0.01643\n",
            "Epoch: 0036 train_loss= 1.50545 train_acc= 0.82500 val_loss= 1.72724 val_acc= 0.58800 time= 0.01608\n",
            "Epoch: 0037 train_loss= 1.50088 train_acc= 0.82500 val_loss= 1.72495 val_acc= 0.60000 time= 0.01575\n",
            "Epoch: 0038 train_loss= 1.49271 train_acc= 0.81667 val_loss= 1.72274 val_acc= 0.60600 time= 0.01587\n",
            "Epoch: 0039 train_loss= 1.48724 train_acc= 0.85833 val_loss= 1.72037 val_acc= 0.61600 time= 0.01610\n",
            "Epoch: 0040 train_loss= 1.49609 train_acc= 0.87500 val_loss= 1.71792 val_acc= 0.62400 time= 0.01606\n",
            "Epoch: 0041 train_loss= 1.44190 train_acc= 0.89167 val_loss= 1.71535 val_acc= 0.62800 time= 0.01547\n",
            "Epoch: 0042 train_loss= 1.44871 train_acc= 0.91667 val_loss= 1.71261 val_acc= 0.63400 time= 0.01653\n",
            "Epoch: 0043 train_loss= 1.42650 train_acc= 0.88333 val_loss= 1.70976 val_acc= 0.64200 time= 0.01516\n",
            "Epoch: 0044 train_loss= 1.39837 train_acc= 0.87500 val_loss= 1.70682 val_acc= 0.64600 time= 0.01552\n",
            "Epoch: 0045 train_loss= 1.44426 train_acc= 0.86667 val_loss= 1.70393 val_acc= 0.65000 time= 0.01495\n",
            "Epoch: 0046 train_loss= 1.39958 train_acc= 0.85833 val_loss= 1.70104 val_acc= 0.65200 time= 0.01540\n",
            "Epoch: 0047 train_loss= 1.43234 train_acc= 0.87500 val_loss= 1.69813 val_acc= 0.65600 time= 0.01566\n",
            "Epoch: 0048 train_loss= 1.36291 train_acc= 0.87500 val_loss= 1.69527 val_acc= 0.66200 time= 0.01547\n",
            "Epoch: 0049 train_loss= 1.34805 train_acc= 0.90000 val_loss= 1.69258 val_acc= 0.66400 time= 0.01586\n",
            "Epoch: 0050 train_loss= 1.37479 train_acc= 0.86667 val_loss= 1.68978 val_acc= 0.66600 time= 0.01583\n",
            "Epoch: 0051 train_loss= 1.36155 train_acc= 0.85833 val_loss= 1.68694 val_acc= 0.66600 time= 0.01666\n",
            "Epoch: 0052 train_loss= 1.34648 train_acc= 0.89167 val_loss= 1.68417 val_acc= 0.66200 time= 0.01626\n",
            "Epoch: 0053 train_loss= 1.31014 train_acc= 0.90833 val_loss= 1.68137 val_acc= 0.66200 time= 0.01708\n",
            "Epoch: 0054 train_loss= 1.29510 train_acc= 0.92500 val_loss= 1.67833 val_acc= 0.66800 time= 0.01550\n",
            "Epoch: 0055 train_loss= 1.34262 train_acc= 0.85833 val_loss= 1.67515 val_acc= 0.67000 time= 0.01545\n",
            "Epoch: 0056 train_loss= 1.30120 train_acc= 0.85000 val_loss= 1.67204 val_acc= 0.67400 time= 0.01549\n",
            "Epoch: 0057 train_loss= 1.30541 train_acc= 0.92500 val_loss= 1.66888 val_acc= 0.67400 time= 0.01574\n",
            "Epoch: 0058 train_loss= 1.28919 train_acc= 0.91667 val_loss= 1.66570 val_acc= 0.67800 time= 0.02150\n",
            "Epoch: 0059 train_loss= 1.29481 train_acc= 0.90000 val_loss= 1.66253 val_acc= 0.67600 time= 0.01600\n",
            "Epoch: 0060 train_loss= 1.25993 train_acc= 0.87500 val_loss= 1.65939 val_acc= 0.68000 time= 0.01552\n",
            "Epoch: 0061 train_loss= 1.21851 train_acc= 0.88333 val_loss= 1.65612 val_acc= 0.68400 time= 0.01574\n",
            "Epoch: 0062 train_loss= 1.26304 train_acc= 0.90833 val_loss= 1.65294 val_acc= 0.68800 time= 0.01600\n",
            "Epoch: 0063 train_loss= 1.20688 train_acc= 0.90833 val_loss= 1.64986 val_acc= 0.69000 time= 0.01769\n",
            "Epoch: 0064 train_loss= 1.25773 train_acc= 0.86667 val_loss= 1.64671 val_acc= 0.68600 time= 0.01670\n",
            "Epoch: 0065 train_loss= 1.22363 train_acc= 0.90000 val_loss= 1.64367 val_acc= 0.68800 time= 0.01712\n",
            "Epoch: 0066 train_loss= 1.20809 train_acc= 0.91667 val_loss= 1.64040 val_acc= 0.68600 time= 0.01695\n",
            "Epoch: 0067 train_loss= 1.23203 train_acc= 0.90000 val_loss= 1.63731 val_acc= 0.69200 time= 0.01630\n",
            "Epoch: 0068 train_loss= 1.24000 train_acc= 0.90000 val_loss= 1.63409 val_acc= 0.70000 time= 0.01583\n",
            "Epoch: 0069 train_loss= 1.19273 train_acc= 0.92500 val_loss= 1.63074 val_acc= 0.70200 time= 0.01557\n",
            "Epoch: 0070 train_loss= 1.17086 train_acc= 0.93333 val_loss= 1.62725 val_acc= 0.70200 time= 0.01588\n",
            "Epoch: 0071 train_loss= 1.15551 train_acc= 0.89167 val_loss= 1.62339 val_acc= 0.70800 time= 0.01899\n",
            "Epoch: 0072 train_loss= 1.15752 train_acc= 0.90000 val_loss= 1.61952 val_acc= 0.70600 time= 0.01610\n",
            "Epoch: 0073 train_loss= 1.19858 train_acc= 0.89167 val_loss= 1.61553 val_acc= 0.70600 time= 0.01621\n",
            "Epoch: 0074 train_loss= 1.10108 train_acc= 0.94167 val_loss= 1.61172 val_acc= 0.70200 time= 0.01546\n",
            "Epoch: 0075 train_loss= 1.16521 train_acc= 0.90000 val_loss= 1.60784 val_acc= 0.70400 time= 0.01880\n",
            "Epoch: 0076 train_loss= 1.09940 train_acc= 0.92500 val_loss= 1.60387 val_acc= 0.70600 time= 0.01997\n",
            "Epoch: 0077 train_loss= 1.10941 train_acc= 0.90833 val_loss= 1.59996 val_acc= 0.70600 time= 0.01627\n",
            "Epoch: 0078 train_loss= 1.08350 train_acc= 0.94167 val_loss= 1.59630 val_acc= 0.70400 time= 0.01668\n",
            "Epoch: 0079 train_loss= 1.08785 train_acc= 0.93333 val_loss= 1.59231 val_acc= 0.70600 time= 0.02230\n",
            "Epoch: 0080 train_loss= 1.10498 train_acc= 0.90000 val_loss= 1.58822 val_acc= 0.70800 time= 0.01653\n",
            "Epoch: 0081 train_loss= 1.09439 train_acc= 0.91667 val_loss= 1.58435 val_acc= 0.70800 time= 0.01604\n",
            "Epoch: 0082 train_loss= 1.10942 train_acc= 0.89167 val_loss= 1.58049 val_acc= 0.70800 time= 0.01638\n",
            "Epoch: 0083 train_loss= 1.09979 train_acc= 0.90833 val_loss= 1.57667 val_acc= 0.70800 time= 0.01624\n",
            "Epoch: 0084 train_loss= 1.09250 train_acc= 0.90833 val_loss= 1.57283 val_acc= 0.70800 time= 0.01651\n",
            "Epoch: 0085 train_loss= 1.10973 train_acc= 0.90833 val_loss= 1.56898 val_acc= 0.71000 time= 0.01593\n",
            "Epoch: 0086 train_loss= 1.04418 train_acc= 0.88333 val_loss= 1.56520 val_acc= 0.71200 time= 0.01568\n",
            "Epoch: 0087 train_loss= 1.09161 train_acc= 0.91667 val_loss= 1.56167 val_acc= 0.71400 time= 0.01663\n",
            "Epoch: 0088 train_loss= 1.05027 train_acc= 0.91667 val_loss= 1.55818 val_acc= 0.71400 time= 0.02119\n",
            "Epoch: 0089 train_loss= 1.03529 train_acc= 0.92500 val_loss= 1.55444 val_acc= 0.71600 time= 0.01750\n",
            "Epoch: 0090 train_loss= 1.06605 train_acc= 0.90833 val_loss= 1.55047 val_acc= 0.71400 time= 0.01596\n",
            "Epoch: 0091 train_loss= 1.03416 train_acc= 0.91667 val_loss= 1.54683 val_acc= 0.71400 time= 0.01590\n",
            "Epoch: 0092 train_loss= 1.02983 train_acc= 0.92500 val_loss= 1.54358 val_acc= 0.71600 time= 0.01549\n",
            "Epoch: 0093 train_loss= 0.99916 train_acc= 0.92500 val_loss= 1.54007 val_acc= 0.71800 time= 0.01574\n",
            "Epoch: 0094 train_loss= 1.03764 train_acc= 0.93333 val_loss= 1.53651 val_acc= 0.72000 time= 0.01625\n",
            "Epoch: 0095 train_loss= 1.02584 train_acc= 0.90833 val_loss= 1.53304 val_acc= 0.72400 time= 0.01575\n",
            "Epoch: 0096 train_loss= 1.00237 train_acc= 0.92500 val_loss= 1.52966 val_acc= 0.72400 time= 0.01566\n",
            "Epoch: 0097 train_loss= 0.97951 train_acc= 0.94167 val_loss= 1.52624 val_acc= 0.72600 time= 0.01536\n",
            "Epoch: 0098 train_loss= 0.95446 train_acc= 0.93333 val_loss= 1.52299 val_acc= 0.72600 time= 0.01542\n",
            "Epoch: 0099 train_loss= 1.00050 train_acc= 0.91667 val_loss= 1.52016 val_acc= 0.72600 time= 0.01628\n",
            "Epoch: 0100 train_loss= 0.94691 train_acc= 0.94167 val_loss= 1.51718 val_acc= 0.72800 time= 0.01554\n",
            "Epoch: 0101 train_loss= 0.96234 train_acc= 0.95000 val_loss= 1.51430 val_acc= 0.72600 time= 0.02048\n",
            "Epoch: 0102 train_loss= 0.98754 train_acc= 0.95000 val_loss= 1.51165 val_acc= 0.72200 time= 0.01593\n",
            "Epoch: 0103 train_loss= 0.97681 train_acc= 0.92500 val_loss= 1.50919 val_acc= 0.72200 time= 0.01508\n",
            "Epoch: 0104 train_loss= 0.97884 train_acc= 0.94167 val_loss= 1.50685 val_acc= 0.72000 time= 0.01562\n",
            "Epoch: 0105 train_loss= 0.95800 train_acc= 0.94167 val_loss= 1.50461 val_acc= 0.72200 time= 0.01641\n",
            "Epoch: 0106 train_loss= 0.94245 train_acc= 0.94167 val_loss= 1.50204 val_acc= 0.72200 time= 0.01593\n",
            "Epoch: 0107 train_loss= 0.96517 train_acc= 0.94167 val_loss= 1.49942 val_acc= 0.72000 time= 0.01592\n",
            "Epoch: 0108 train_loss= 0.96410 train_acc= 0.94167 val_loss= 1.49658 val_acc= 0.71600 time= 0.01617\n",
            "Epoch: 0109 train_loss= 0.90230 train_acc= 0.94167 val_loss= 1.49396 val_acc= 0.71600 time= 0.01688\n",
            "Epoch: 0110 train_loss= 0.95621 train_acc= 0.92500 val_loss= 1.49140 val_acc= 0.71400 time= 0.02526\n",
            "Epoch: 0111 train_loss= 0.93252 train_acc= 0.94167 val_loss= 1.48885 val_acc= 0.71400 time= 0.01585\n",
            "Epoch: 0112 train_loss= 0.93293 train_acc= 0.91667 val_loss= 1.48653 val_acc= 0.71600 time= 0.01605\n",
            "Epoch: 0113 train_loss= 0.92845 train_acc= 0.94167 val_loss= 1.48404 val_acc= 0.71600 time= 0.01577\n",
            "Epoch: 0114 train_loss= 0.85856 train_acc= 0.94167 val_loss= 1.48151 val_acc= 0.71600 time= 0.01554\n",
            "Epoch: 0115 train_loss= 0.94061 train_acc= 0.92500 val_loss= 1.47876 val_acc= 0.71600 time= 0.01525\n",
            "Epoch: 0116 train_loss= 0.92028 train_acc= 0.97500 val_loss= 1.47591 val_acc= 0.71600 time= 0.01577\n",
            "Epoch: 0117 train_loss= 0.89018 train_acc= 0.95000 val_loss= 1.47293 val_acc= 0.71800 time= 0.01609\n",
            "Epoch: 0118 train_loss= 0.88501 train_acc= 0.94167 val_loss= 1.46991 val_acc= 0.71400 time= 0.01557\n",
            "Epoch: 0119 train_loss= 0.88696 train_acc= 0.91667 val_loss= 1.46679 val_acc= 0.71800 time= 0.01870\n",
            "Epoch: 0120 train_loss= 0.86557 train_acc= 0.93333 val_loss= 1.46422 val_acc= 0.71800 time= 0.01566\n",
            "Epoch: 0121 train_loss= 0.92095 train_acc= 0.93333 val_loss= 1.46144 val_acc= 0.72000 time= 0.01475\n",
            "Epoch: 0122 train_loss= 0.89489 train_acc= 0.93333 val_loss= 1.45846 val_acc= 0.72000 time= 0.01552\n",
            "Epoch: 0123 train_loss= 0.87675 train_acc= 0.95833 val_loss= 1.45569 val_acc= 0.72000 time= 0.01712\n",
            "Epoch: 0124 train_loss= 0.87881 train_acc= 0.91667 val_loss= 1.45311 val_acc= 0.72000 time= 0.01585\n",
            "Epoch: 0125 train_loss= 0.89132 train_acc= 0.96667 val_loss= 1.45070 val_acc= 0.71800 time= 0.01661\n",
            "Epoch: 0126 train_loss= 0.89525 train_acc= 0.93333 val_loss= 1.44905 val_acc= 0.71800 time= 0.01620\n",
            "Epoch: 0127 train_loss= 0.87683 train_acc= 0.94167 val_loss= 1.44741 val_acc= 0.71800 time= 0.01568\n",
            "Epoch: 0128 train_loss= 0.88006 train_acc= 0.97500 val_loss= 1.44526 val_acc= 0.71600 time= 0.01532\n",
            "Epoch: 0129 train_loss= 0.86274 train_acc= 0.92500 val_loss= 1.44349 val_acc= 0.71400 time= 0.01523\n",
            "Epoch: 0130 train_loss= 0.90465 train_acc= 0.90000 val_loss= 1.44171 val_acc= 0.71000 time= 0.01575\n",
            "Epoch: 0131 train_loss= 0.87505 train_acc= 0.95833 val_loss= 1.43980 val_acc= 0.71000 time= 0.01571\n",
            "Epoch: 0132 train_loss= 0.87579 train_acc= 0.93333 val_loss= 1.43792 val_acc= 0.70800 time= 0.01590\n",
            "Epoch: 0133 train_loss= 0.87283 train_acc= 0.94167 val_loss= 1.43570 val_acc= 0.71000 time= 0.01593\n",
            "Epoch: 0134 train_loss= 0.88446 train_acc= 0.95000 val_loss= 1.43323 val_acc= 0.70800 time= 0.01592\n",
            "Epoch: 0135 train_loss= 0.86175 train_acc= 0.90833 val_loss= 1.43125 val_acc= 0.70600 time= 0.01542\n",
            "Epoch: 0136 train_loss= 0.82939 train_acc= 0.92500 val_loss= 1.42975 val_acc= 0.70600 time= 0.01537\n",
            "Epoch: 0137 train_loss= 0.83769 train_acc= 0.93333 val_loss= 1.42851 val_acc= 0.70600 time= 0.02218\n",
            "Epoch: 0138 train_loss= 0.86683 train_acc= 0.92500 val_loss= 1.42740 val_acc= 0.70600 time= 0.01815\n",
            "Epoch: 0139 train_loss= 0.85935 train_acc= 0.91667 val_loss= 1.42597 val_acc= 0.70600 time= 0.01647\n",
            "Epoch: 0140 train_loss= 0.81034 train_acc= 0.96667 val_loss= 1.42457 val_acc= 0.70600 time= 0.01667\n",
            "Epoch: 0141 train_loss= 0.83527 train_acc= 0.96667 val_loss= 1.42278 val_acc= 0.70600 time= 0.01754\n",
            "Epoch: 0142 train_loss= 0.85731 train_acc= 0.92500 val_loss= 1.42074 val_acc= 0.70600 time= 0.01637\n",
            "Epoch: 0143 train_loss= 0.83532 train_acc= 0.93333 val_loss= 1.41871 val_acc= 0.70600 time= 0.01629\n",
            "Epoch: 0144 train_loss= 0.78013 train_acc= 0.95833 val_loss= 1.41625 val_acc= 0.71200 time= 0.01779\n",
            "Epoch: 0145 train_loss= 0.85737 train_acc= 0.94167 val_loss= 1.41365 val_acc= 0.71600 time= 0.01578\n",
            "Epoch: 0146 train_loss= 0.82868 train_acc= 0.95833 val_loss= 1.41107 val_acc= 0.72000 time= 0.01582\n",
            "Epoch: 0147 train_loss= 0.79201 train_acc= 0.95000 val_loss= 1.40842 val_acc= 0.71800 time= 0.01573\n",
            "Epoch: 0148 train_loss= 0.75202 train_acc= 0.95833 val_loss= 1.40592 val_acc= 0.71800 time= 0.01615\n",
            "Epoch: 0149 train_loss= 0.83302 train_acc= 0.94167 val_loss= 1.40343 val_acc= 0.71800 time= 0.01771\n",
            "Epoch: 0150 train_loss= 0.79850 train_acc= 0.95000 val_loss= 1.40103 val_acc= 0.71800 time= 0.01621\n",
            "Epoch: 0151 train_loss= 0.81267 train_acc= 0.93333 val_loss= 1.39887 val_acc= 0.71600 time= 0.01599\n",
            "Epoch: 0152 train_loss= 0.79709 train_acc= 0.95833 val_loss= 1.39722 val_acc= 0.71400 time= 0.01595\n",
            "Epoch: 0153 train_loss= 0.81040 train_acc= 0.90833 val_loss= 1.39653 val_acc= 0.71200 time= 0.01644\n",
            "Epoch: 0154 train_loss= 0.79175 train_acc= 0.95000 val_loss= 1.39556 val_acc= 0.71200 time= 0.01552\n",
            "Epoch: 0155 train_loss= 0.80458 train_acc= 0.95833 val_loss= 1.39467 val_acc= 0.71400 time= 0.01597\n",
            "Epoch: 0156 train_loss= 0.76733 train_acc= 0.95833 val_loss= 1.39358 val_acc= 0.71600 time= 0.01590\n",
            "Epoch: 0157 train_loss= 0.76674 train_acc= 0.98333 val_loss= 1.39158 val_acc= 0.71600 time= 0.02171\n",
            "Epoch: 0158 train_loss= 0.77807 train_acc= 0.95833 val_loss= 1.38940 val_acc= 0.71600 time= 0.01626\n",
            "Epoch: 0159 train_loss= 0.77890 train_acc= 0.95833 val_loss= 1.38700 val_acc= 0.71600 time= 0.01630\n",
            "Epoch: 0160 train_loss= 0.76745 train_acc= 0.93333 val_loss= 1.38464 val_acc= 0.71600 time= 0.01647\n",
            "Epoch: 0161 train_loss= 0.79779 train_acc= 0.95833 val_loss= 1.38284 val_acc= 0.71800 time= 0.01664\n",
            "Epoch: 0162 train_loss= 0.75311 train_acc= 0.95833 val_loss= 1.38111 val_acc= 0.72000 time= 0.01585\n",
            "Epoch: 0163 train_loss= 0.79390 train_acc= 0.93333 val_loss= 1.38015 val_acc= 0.72200 time= 0.01563\n",
            "Epoch: 0164 train_loss= 0.81050 train_acc= 0.91667 val_loss= 1.37943 val_acc= 0.72000 time= 0.01541\n",
            "Epoch: 0165 train_loss= 0.81914 train_acc= 0.92500 val_loss= 1.37872 val_acc= 0.71800 time= 0.01529\n",
            "Epoch: 0166 train_loss= 0.75343 train_acc= 0.95000 val_loss= 1.37790 val_acc= 0.71600 time= 0.01522\n",
            "Epoch: 0167 train_loss= 0.74610 train_acc= 0.95000 val_loss= 1.37679 val_acc= 0.71800 time= 0.02082\n",
            "Epoch: 0168 train_loss= 0.74992 train_acc= 0.97500 val_loss= 1.37538 val_acc= 0.71600 time= 0.01596\n",
            "Epoch: 0169 train_loss= 0.74308 train_acc= 0.95833 val_loss= 1.37405 val_acc= 0.71600 time= 0.01568\n",
            "Epoch: 0170 train_loss= 0.74567 train_acc= 0.93333 val_loss= 1.37287 val_acc= 0.71600 time= 0.01523\n",
            "Epoch: 0171 train_loss= 0.73610 train_acc= 0.95000 val_loss= 1.37234 val_acc= 0.71200 time= 0.01737\n",
            "Epoch: 0172 train_loss= 0.76316 train_acc= 0.95000 val_loss= 1.37216 val_acc= 0.71200 time= 0.01583\n",
            "Epoch: 0173 train_loss= 0.73378 train_acc= 0.93333 val_loss= 1.37228 val_acc= 0.71000 time= 0.01566\n",
            "Epoch: 0174 train_loss= 0.74988 train_acc= 0.94167 val_loss= 1.37224 val_acc= 0.71200 time= 0.01561\n",
            "Epoch: 0175 train_loss= 0.77180 train_acc= 0.92500 val_loss= 1.37229 val_acc= 0.71000 time= 0.01557\n",
            "Epoch: 0176 train_loss= 0.70896 train_acc= 0.95000 val_loss= 1.37158 val_acc= 0.70800 time= 0.01556\n",
            "Epoch: 0177 train_loss= 0.73037 train_acc= 0.95000 val_loss= 1.37039 val_acc= 0.70400 time= 0.01633\n",
            "Epoch: 0178 train_loss= 0.75322 train_acc= 0.95000 val_loss= 1.36920 val_acc= 0.70600 time= 0.01566\n",
            "Epoch: 0179 train_loss= 0.73102 train_acc= 0.95833 val_loss= 1.36774 val_acc= 0.70800 time= 0.01529\n",
            "Epoch: 0180 train_loss= 0.76019 train_acc= 0.95833 val_loss= 1.36545 val_acc= 0.70800 time= 0.02428\n",
            "Epoch: 0181 train_loss= 0.74087 train_acc= 0.93333 val_loss= 1.36251 val_acc= 0.71000 time= 0.01605\n",
            "Epoch: 0182 train_loss= 0.75393 train_acc= 0.92500 val_loss= 1.35965 val_acc= 0.70800 time= 0.01562\n",
            "Epoch: 0183 train_loss= 0.74553 train_acc= 0.95833 val_loss= 1.35684 val_acc= 0.71000 time= 0.01762\n",
            "Epoch: 0184 train_loss= 0.70943 train_acc= 0.92500 val_loss= 1.35362 val_acc= 0.71000 time= 0.01645\n",
            "Epoch: 0185 train_loss= 0.72471 train_acc= 0.93333 val_loss= 1.35057 val_acc= 0.71200 time= 0.01681\n",
            "Epoch: 0186 train_loss= 0.75579 train_acc= 0.92500 val_loss= 1.34833 val_acc= 0.71200 time= 0.01666\n",
            "Epoch: 0187 train_loss= 0.72439 train_acc= 0.95000 val_loss= 1.34586 val_acc= 0.71200 time= 0.01634\n",
            "Epoch: 0188 train_loss= 0.74390 train_acc= 0.91667 val_loss= 1.34289 val_acc= 0.71200 time= 0.01635\n",
            "Epoch: 0189 train_loss= 0.72597 train_acc= 0.94167 val_loss= 1.34059 val_acc= 0.71200 time= 0.01557\n",
            "Epoch: 0190 train_loss= 0.72168 train_acc= 0.95833 val_loss= 1.33877 val_acc= 0.71000 time= 0.01626\n",
            "Epoch: 0191 train_loss= 0.73265 train_acc= 0.92500 val_loss= 1.33654 val_acc= 0.70800 time= 0.01558\n",
            "Epoch: 0192 train_loss= 0.66658 train_acc= 0.98333 val_loss= 1.33545 val_acc= 0.70200 time= 0.01929\n",
            "Epoch: 0193 train_loss= 0.70897 train_acc= 0.95833 val_loss= 1.33458 val_acc= 0.70800 time= 0.01537\n",
            "Epoch: 0194 train_loss= 0.70495 train_acc= 0.93333 val_loss= 1.33387 val_acc= 0.71200 time= 0.01546\n",
            "Epoch: 0195 train_loss= 0.72222 train_acc= 0.93333 val_loss= 1.33362 val_acc= 0.71200 time= 0.01575\n",
            "Epoch: 0196 train_loss= 0.66045 train_acc= 0.95833 val_loss= 1.33316 val_acc= 0.71000 time= 0.01701\n",
            "Epoch: 0197 train_loss= 0.74128 train_acc= 0.94167 val_loss= 1.33336 val_acc= 0.71000 time= 0.01563\n",
            "Epoch: 0198 train_loss= 0.69613 train_acc= 0.95833 val_loss= 1.33314 val_acc= 0.70800 time= 0.02429\n",
            "Epoch: 0199 train_loss= 0.68806 train_acc= 0.95000 val_loss= 1.33301 val_acc= 0.70800 time= 0.01844\n",
            "Epoch: 0200 train_loss= 0.71659 train_acc= 0.93333 val_loss= 1.33291 val_acc= 0.71000 time= 0.01575\n",
            "Optimization Finished!\n",
            "Test set results: cost= 1.30984 accuracy= 0.70600 time= 0.00931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "x, y = [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_x = pca.fit_transform(x)\n",
        "\n",
        "color = ['blue', 'red']\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def update(frame):\n",
        "    ax.cla()\n",
        "    for index, item in enumerate(reduced_x):\n",
        "        if index < len(y):\n",
        "            label_index = min(y[index], len(color) - 1)\n",
        "            ax.scatter(item[0], item[1], c=color[label_index])\n",
        "    ax.set_title(\"GCN Visualization\")\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(reduced_x), interval=200)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8DiVYdLh8rZq",
        "outputId": "cec6425c-05ed-48bd-d00d-8a09af59e2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/matplotlib/animation.py:884: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you output the Animation using `plt.show()` or `anim.save()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkS0HSU1SVl",
        "outputId": "e8a3f937-4f65-442f-fece-18c6a9c736cb"
      },
      "source": [
        "import matplotlib.pyplot as plt                 #matplotlib\n",
        "from sklearn.decomposition import PCA           #PCA\n",
        "\n",
        "x, y= [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:  # \n",
        "    data1 = f.read().strip().split(\"\\n\")  # \n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:  # \n",
        "    data1 = f.read().strip().split(\"\\n\")  # \n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca=PCA(n_components=2)     #PCA2\n",
        "reduced_x=pca.fit_transform(x)#\n",
        "print(reduced_x)\n",
        "\n",
        "# #\n",
        "color = ['blue', 'red']\n",
        "for index, item in enumerate(reduced_x):\n",
        "    label_index = min(y[index], len(color) - 1)\n",
        "    plt.scatter(item[0], item[1], c=color[label_index])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.02969309  1.92668251]\n",
            " [-2.87816948 -1.51689346]\n",
            " [ 5.31122672 -1.60693644]\n",
            " ...\n",
            " [-0.6515273   2.00398085]\n",
            " [-0.96841437 -1.45814141]\n",
            " [ 1.08727754 -2.01871838]]\n"
          ]
        }
      ]
    }
  ]
}