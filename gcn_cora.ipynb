{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_cora.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHIN-HUA/gcn_colab/blob/main/gcn_cora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hengqujushi/gcn_colab"
      ],
      "metadata": {
        "id": "WibmtcrJwTuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXFgbAudsyzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8be435-1a3c-48b9-8f0a-fb34813d6537"
      },
      "source": [
        "!python ./gcn_colab/setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating gcn.egg-info\n",
            "writing gcn.egg-info/PKG-INFO\n",
            "writing dependency_links to gcn.egg-info/dependency_links.txt\n",
            "writing requirements to gcn.egg-info/requires.txt\n",
            "writing top-level names to gcn.egg-info/top_level.txt\n",
            "writing manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "reading manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "writing manifest file 'gcn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying gcn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/gcn-1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing gcn-1.0-py3.10.egg\n",
            "Copying gcn-1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding gcn 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/gcn-1.0-py3.10.egg\n",
            "Processing dependencies for gcn==1.0\n",
            "Searching for scipy==1.10.1\n",
            "Best match: scipy 1.10.1\n",
            "Adding scipy 1.10.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.1\n",
            "Best match: networkx 3.1\n",
            "Adding networkx 3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.22.4\n",
            "Best match: numpy 1.22.4\n",
            "Adding numpy 1.22.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.10 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for gcn==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSLPHNbGs49u",
        "outputId": "4aa2ab7d-8211-42c7-c2f1-1eeb8247a800"
      },
      "source": [
        "\n",
        "!cd ./gcn_colab/gcn && python train.py --dataset pubmed\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-16 08:45:35.297714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-16 08:45:36.281913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-16 08:45:38.557351: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-16 08:45:38.557510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
            "\t [[{{node Placeholder_7}}]]\n",
            "2023-05-16 08:45:38.567592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.567707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2023-05-16 08:45:38.576744: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.576854: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2023-05-16 08:45:38.643893: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.644017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.671325: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.671443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-16 08:45:38.741205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:38.771684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:38.771934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:38.772647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:38.772866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:38.773069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:39.778696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:39.778977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:39.779184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-16 08:45:39.779322: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-16 08:45:39.779364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-05-16 08:45:39.782940: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "Epoch: 0001 train_loss= 1.10632 train_acc= 0.35000 val_loss= 1.10392 val_acc= 0.55400 time= 1.14241\n",
            "Epoch: 0002 train_loss= 1.10202 train_acc= 0.68333 val_loss= 1.10140 val_acc= 0.68400 time= 0.16396\n",
            "Epoch: 0003 train_loss= 1.09728 train_acc= 0.73333 val_loss= 1.09828 val_acc= 0.71600 time= 0.15179\n",
            "Epoch: 0004 train_loss= 1.09206 train_acc= 0.78333 val_loss= 1.09468 val_acc= 0.71200 time= 0.14707\n",
            "Epoch: 0005 train_loss= 1.08634 train_acc= 0.90000 val_loss= 1.09076 val_acc= 0.70600 time= 0.15013\n",
            "Epoch: 0006 train_loss= 1.08144 train_acc= 0.83333 val_loss= 1.08682 val_acc= 0.70800 time= 0.13584\n",
            "Epoch: 0007 train_loss= 1.07485 train_acc= 0.81667 val_loss= 1.08285 val_acc= 0.70800 time= 0.13597\n",
            "Epoch: 0008 train_loss= 1.06455 train_acc= 0.83333 val_loss= 1.07880 val_acc= 0.71600 time= 0.14142\n",
            "Epoch: 0009 train_loss= 1.05632 train_acc= 0.88333 val_loss= 1.07467 val_acc= 0.71000 time= 0.13554\n",
            "Epoch: 0010 train_loss= 1.05322 train_acc= 0.85000 val_loss= 1.07044 val_acc= 0.70400 time= 0.11433\n",
            "Epoch: 0011 train_loss= 1.03936 train_acc= 0.86667 val_loss= 1.06618 val_acc= 0.71000 time= 0.11421\n",
            "Epoch: 0012 train_loss= 1.03746 train_acc= 0.86667 val_loss= 1.06179 val_acc= 0.71000 time= 0.11408\n",
            "Epoch: 0013 train_loss= 1.01769 train_acc= 0.88333 val_loss= 1.05735 val_acc= 0.70800 time= 0.11191\n",
            "Epoch: 0014 train_loss= 1.01573 train_acc= 0.90000 val_loss= 1.05291 val_acc= 0.71600 time= 0.11181\n",
            "Epoch: 0015 train_loss= 1.00745 train_acc= 0.86667 val_loss= 1.04846 val_acc= 0.71600 time= 0.10274\n",
            "Epoch: 0016 train_loss= 1.00863 train_acc= 0.86667 val_loss= 1.04407 val_acc= 0.72200 time= 0.11003\n",
            "Epoch: 0017 train_loss= 0.99092 train_acc= 0.86667 val_loss= 1.03956 val_acc= 0.72400 time= 0.11282\n",
            "Epoch: 0018 train_loss= 0.97307 train_acc= 0.88333 val_loss= 1.03489 val_acc= 0.72600 time= 0.10524\n",
            "Epoch: 0019 train_loss= 0.96775 train_acc= 0.88333 val_loss= 1.03016 val_acc= 0.72200 time= 0.11896\n",
            "Epoch: 0020 train_loss= 0.95938 train_acc= 0.90000 val_loss= 1.02527 val_acc= 0.72400 time= 0.10761\n",
            "Epoch: 0021 train_loss= 0.94814 train_acc= 0.86667 val_loss= 1.02038 val_acc= 0.72200 time= 0.10648\n",
            "Epoch: 0022 train_loss= 0.92506 train_acc= 0.90000 val_loss= 1.01540 val_acc= 0.72000 time= 0.10345\n",
            "Epoch: 0023 train_loss= 0.92615 train_acc= 0.93333 val_loss= 1.01045 val_acc= 0.72000 time= 0.11862\n",
            "Epoch: 0024 train_loss= 0.91979 train_acc= 0.91667 val_loss= 1.00540 val_acc= 0.72000 time= 0.10679\n",
            "Epoch: 0025 train_loss= 0.90065 train_acc= 0.90000 val_loss= 1.00008 val_acc= 0.72000 time= 0.12169\n",
            "Epoch: 0026 train_loss= 0.88484 train_acc= 0.90000 val_loss= 0.99469 val_acc= 0.72200 time= 0.10117\n",
            "Epoch: 0027 train_loss= 0.89073 train_acc= 0.90000 val_loss= 0.98936 val_acc= 0.72400 time= 0.10091\n",
            "Epoch: 0028 train_loss= 0.86079 train_acc= 0.93333 val_loss= 0.98401 val_acc= 0.73000 time= 0.10059\n",
            "Epoch: 0029 train_loss= 0.84093 train_acc= 0.91667 val_loss= 0.97866 val_acc= 0.73200 time= 0.09833\n",
            "Epoch: 0030 train_loss= 0.83643 train_acc= 0.88333 val_loss= 0.97335 val_acc= 0.73200 time= 0.10574\n",
            "Epoch: 0031 train_loss= 0.84596 train_acc= 0.91667 val_loss= 0.96799 val_acc= 0.73400 time= 0.09989\n",
            "Epoch: 0032 train_loss= 0.83667 train_acc= 0.86667 val_loss= 0.96257 val_acc= 0.73800 time= 0.09886\n",
            "Epoch: 0033 train_loss= 0.81335 train_acc= 0.93333 val_loss= 0.95699 val_acc= 0.73400 time= 0.12708\n",
            "Epoch: 0034 train_loss= 0.80183 train_acc= 0.93333 val_loss= 0.95148 val_acc= 0.73600 time= 0.09920\n",
            "Epoch: 0035 train_loss= 0.78570 train_acc= 0.93333 val_loss= 0.94611 val_acc= 0.74000 time= 0.11592\n",
            "Epoch: 0036 train_loss= 0.78355 train_acc= 0.90000 val_loss= 0.94085 val_acc= 0.74400 time= 0.10987\n",
            "Epoch: 0037 train_loss= 0.75434 train_acc= 0.93333 val_loss= 0.93566 val_acc= 0.74600 time= 0.11559\n",
            "Epoch: 0038 train_loss= 0.73122 train_acc= 0.96667 val_loss= 0.93071 val_acc= 0.74600 time= 0.11601\n",
            "Epoch: 0039 train_loss= 0.72960 train_acc= 0.93333 val_loss= 0.92583 val_acc= 0.74800 time= 0.11304\n",
            "Epoch: 0040 train_loss= 0.74719 train_acc= 0.93333 val_loss= 0.92103 val_acc= 0.74800 time= 0.09973\n",
            "Epoch: 0041 train_loss= 0.71352 train_acc= 0.91667 val_loss= 0.91629 val_acc= 0.75000 time= 0.10467\n",
            "Epoch: 0042 train_loss= 0.72780 train_acc= 0.90000 val_loss= 0.91152 val_acc= 0.75000 time= 0.11024\n",
            "Epoch: 0043 train_loss= 0.71417 train_acc= 0.90000 val_loss= 0.90675 val_acc= 0.75800 time= 0.10228\n",
            "Epoch: 0044 train_loss= 0.68722 train_acc= 0.95000 val_loss= 0.90207 val_acc= 0.75200 time= 0.11497\n",
            "Epoch: 0045 train_loss= 0.66771 train_acc= 0.95000 val_loss= 0.89753 val_acc= 0.75200 time= 0.11629\n",
            "Epoch: 0046 train_loss= 0.68004 train_acc= 0.95000 val_loss= 0.89296 val_acc= 0.75400 time= 0.12227\n",
            "Epoch: 0047 train_loss= 0.67649 train_acc= 0.91667 val_loss= 0.88857 val_acc= 0.75600 time= 0.11671\n",
            "Epoch: 0048 train_loss= 0.65185 train_acc= 0.95000 val_loss= 0.88420 val_acc= 0.75800 time= 0.10273\n",
            "Epoch: 0049 train_loss= 0.65603 train_acc= 0.96667 val_loss= 0.87993 val_acc= 0.76000 time= 0.11112\n",
            "Epoch: 0050 train_loss= 0.64271 train_acc= 0.90000 val_loss= 0.87566 val_acc= 0.76200 time= 0.10278\n",
            "Epoch: 0051 train_loss= 0.63248 train_acc= 0.90000 val_loss= 0.87140 val_acc= 0.76400 time= 0.11420\n",
            "Epoch: 0052 train_loss= 0.61949 train_acc= 0.95000 val_loss= 0.86746 val_acc= 0.76600 time= 0.11643\n",
            "Epoch: 0053 train_loss= 0.58295 train_acc= 0.95000 val_loss= 0.86357 val_acc= 0.76800 time= 0.11534\n",
            "Epoch: 0054 train_loss= 0.61595 train_acc= 0.91667 val_loss= 0.85958 val_acc= 0.77000 time= 0.11974\n",
            "Epoch: 0055 train_loss= 0.59371 train_acc= 0.98333 val_loss= 0.85588 val_acc= 0.76800 time= 0.11351\n",
            "Epoch: 0056 train_loss= 0.59344 train_acc= 0.96667 val_loss= 0.85221 val_acc= 0.77400 time= 0.10277\n",
            "Epoch: 0057 train_loss= 0.57720 train_acc= 0.98333 val_loss= 0.84877 val_acc= 0.77400 time= 0.10893\n",
            "Epoch: 0058 train_loss= 0.57945 train_acc= 0.95000 val_loss= 0.84543 val_acc= 0.77600 time= 0.10260\n",
            "Epoch: 0059 train_loss= 0.52443 train_acc= 0.96667 val_loss= 0.84238 val_acc= 0.77600 time= 0.12071\n",
            "Epoch: 0060 train_loss= 0.55858 train_acc= 0.96667 val_loss= 0.83934 val_acc= 0.78600 time= 0.10724\n",
            "Epoch: 0061 train_loss= 0.55906 train_acc= 0.95000 val_loss= 0.83625 val_acc= 0.78600 time= 0.11283\n",
            "Epoch: 0062 train_loss= 0.59157 train_acc= 0.91667 val_loss= 0.83332 val_acc= 0.78800 time= 0.11712\n",
            "Epoch: 0063 train_loss= 0.59376 train_acc= 0.90000 val_loss= 0.83034 val_acc= 0.79000 time= 0.12019\n",
            "Epoch: 0064 train_loss= 0.53467 train_acc= 0.95000 val_loss= 0.82746 val_acc= 0.79400 time= 0.10002\n",
            "Epoch: 0065 train_loss= 0.55793 train_acc= 0.95000 val_loss= 0.82440 val_acc= 0.79000 time= 0.10255\n",
            "Epoch: 0066 train_loss= 0.50253 train_acc= 0.98333 val_loss= 0.82145 val_acc= 0.79000 time= 0.09895\n",
            "Epoch: 0067 train_loss= 0.53799 train_acc= 0.98333 val_loss= 0.81855 val_acc= 0.79400 time= 0.10140\n",
            "Epoch: 0068 train_loss= 0.51994 train_acc= 0.96667 val_loss= 0.81597 val_acc= 0.79400 time= 0.12090\n",
            "Epoch: 0069 train_loss= 0.50051 train_acc= 0.95000 val_loss= 0.81350 val_acc= 0.79400 time= 0.13236\n",
            "Epoch: 0070 train_loss= 0.51844 train_acc= 0.95000 val_loss= 0.81119 val_acc= 0.79400 time= 0.12724\n",
            "Epoch: 0071 train_loss= 0.50371 train_acc= 0.95000 val_loss= 0.80882 val_acc= 0.79400 time= 0.11373\n",
            "Epoch: 0072 train_loss= 0.49752 train_acc= 0.93333 val_loss= 0.80604 val_acc= 0.79400 time= 0.10887\n",
            "Epoch: 0073 train_loss= 0.51785 train_acc= 0.95000 val_loss= 0.80355 val_acc= 0.79600 time= 0.10130\n",
            "Epoch: 0074 train_loss= 0.51311 train_acc= 0.96667 val_loss= 0.80119 val_acc= 0.79600 time= 0.11552\n",
            "Epoch: 0075 train_loss= 0.46998 train_acc= 0.98333 val_loss= 0.79894 val_acc= 0.79800 time= 0.10092\n",
            "Epoch: 0076 train_loss= 0.48752 train_acc= 0.95000 val_loss= 0.79684 val_acc= 0.80000 time= 0.11488\n",
            "Epoch: 0077 train_loss= 0.46253 train_acc= 0.96667 val_loss= 0.79484 val_acc= 0.80000 time= 0.10123\n",
            "Epoch: 0078 train_loss= 0.48587 train_acc= 0.95000 val_loss= 0.79309 val_acc= 0.80000 time= 0.10130\n",
            "Epoch: 0079 train_loss= 0.46610 train_acc= 0.96667 val_loss= 0.79165 val_acc= 0.80400 time= 0.12839\n",
            "Epoch: 0080 train_loss= 0.48276 train_acc= 0.98333 val_loss= 0.79034 val_acc= 0.80200 time= 0.11492\n",
            "Epoch: 0081 train_loss= 0.50312 train_acc= 0.98333 val_loss= 0.78869 val_acc= 0.80200 time= 0.11547\n",
            "Epoch: 0082 train_loss= 0.45398 train_acc= 0.96667 val_loss= 0.78674 val_acc= 0.80000 time= 0.10843\n",
            "Epoch: 0083 train_loss= 0.48277 train_acc= 0.98333 val_loss= 0.78469 val_acc= 0.80000 time= 0.10126\n",
            "Epoch: 0084 train_loss= 0.44059 train_acc= 0.96667 val_loss= 0.78265 val_acc= 0.80200 time= 0.09643\n",
            "Epoch: 0085 train_loss= 0.47676 train_acc= 0.95000 val_loss= 0.78053 val_acc= 0.80200 time= 0.09774\n",
            "Epoch: 0086 train_loss= 0.47889 train_acc= 0.98333 val_loss= 0.77853 val_acc= 0.80200 time= 0.10190\n",
            "Epoch: 0087 train_loss= 0.41471 train_acc= 0.95000 val_loss= 0.77655 val_acc= 0.80000 time= 0.09690\n",
            "Epoch: 0088 train_loss= 0.42158 train_acc= 0.96667 val_loss= 0.77484 val_acc= 0.80000 time= 0.11873\n",
            "Epoch: 0089 train_loss= 0.44743 train_acc= 0.96667 val_loss= 0.77343 val_acc= 0.80200 time= 0.09702\n",
            "Epoch: 0090 train_loss= 0.39464 train_acc= 1.00000 val_loss= 0.77191 val_acc= 0.80400 time= 0.09830\n",
            "Epoch: 0091 train_loss= 0.43655 train_acc= 0.96667 val_loss= 0.77048 val_acc= 0.80400 time= 0.11119\n",
            "Epoch: 0092 train_loss= 0.46771 train_acc= 0.95000 val_loss= 0.76930 val_acc= 0.80200 time= 0.11482\n",
            "Epoch: 0093 train_loss= 0.45463 train_acc= 0.96667 val_loss= 0.76814 val_acc= 0.80200 time= 0.12583\n",
            "Epoch: 0094 train_loss= 0.44194 train_acc= 0.96667 val_loss= 0.76674 val_acc= 0.80000 time= 0.11394\n",
            "Epoch: 0095 train_loss= 0.42993 train_acc= 0.96667 val_loss= 0.76533 val_acc= 0.80000 time= 0.11205\n",
            "Epoch: 0096 train_loss= 0.44145 train_acc= 0.93333 val_loss= 0.76417 val_acc= 0.79800 time= 0.11706\n",
            "Epoch: 0097 train_loss= 0.46300 train_acc= 0.95000 val_loss= 0.76310 val_acc= 0.79800 time= 0.12253\n",
            "Epoch: 0098 train_loss= 0.42105 train_acc= 0.96667 val_loss= 0.76180 val_acc= 0.79800 time= 0.11214\n",
            "Epoch: 0099 train_loss= 0.41671 train_acc= 1.00000 val_loss= 0.76032 val_acc= 0.79600 time= 0.10151\n",
            "Epoch: 0100 train_loss= 0.42644 train_acc= 0.98333 val_loss= 0.75902 val_acc= 0.79600 time= 0.11423\n",
            "Epoch: 0101 train_loss= 0.40848 train_acc= 0.98333 val_loss= 0.75808 val_acc= 0.79600 time= 0.15312\n",
            "Epoch: 0102 train_loss= 0.35418 train_acc= 1.00000 val_loss= 0.75695 val_acc= 0.79400 time= 0.13897\n",
            "Epoch: 0103 train_loss= 0.40365 train_acc= 0.96667 val_loss= 0.75618 val_acc= 0.79400 time= 0.14518\n",
            "Epoch: 0104 train_loss= 0.39941 train_acc= 1.00000 val_loss= 0.75557 val_acc= 0.79400 time= 0.13591\n",
            "Epoch: 0105 train_loss= 0.41336 train_acc= 1.00000 val_loss= 0.75488 val_acc= 0.79600 time= 0.15013\n",
            "Epoch: 0106 train_loss= 0.39686 train_acc= 1.00000 val_loss= 0.75371 val_acc= 0.79600 time= 0.12815\n",
            "Epoch: 0107 train_loss= 0.39214 train_acc= 0.98333 val_loss= 0.75293 val_acc= 0.79400 time= 0.14951\n",
            "Epoch: 0108 train_loss= 0.40191 train_acc= 0.98333 val_loss= 0.75226 val_acc= 0.79600 time= 0.14874\n",
            "Epoch: 0109 train_loss= 0.45723 train_acc= 0.93333 val_loss= 0.75173 val_acc= 0.79400 time= 0.14418\n",
            "Epoch: 0110 train_loss= 0.41195 train_acc= 0.96667 val_loss= 0.75145 val_acc= 0.79400 time= 0.14067\n",
            "Epoch: 0111 train_loss= 0.40711 train_acc= 0.98333 val_loss= 0.75133 val_acc= 0.79600 time= 0.13089\n",
            "Epoch: 0112 train_loss= 0.36178 train_acc= 1.00000 val_loss= 0.75101 val_acc= 0.79400 time= 0.13412\n",
            "Epoch: 0113 train_loss= 0.36199 train_acc= 1.00000 val_loss= 0.75034 val_acc= 0.79200 time= 0.13602\n",
            "Epoch: 0114 train_loss= 0.38106 train_acc= 0.98333 val_loss= 0.74953 val_acc= 0.78800 time= 0.13715\n",
            "Epoch: 0115 train_loss= 0.38100 train_acc= 0.98333 val_loss= 0.74884 val_acc= 0.79000 time= 0.13815\n",
            "Epoch: 0116 train_loss= 0.36989 train_acc= 0.98333 val_loss= 0.74819 val_acc= 0.79200 time= 0.13375\n",
            "Epoch: 0117 train_loss= 0.37529 train_acc= 0.96667 val_loss= 0.74782 val_acc= 0.79400 time= 0.14567\n",
            "Epoch: 0118 train_loss= 0.41692 train_acc= 0.96667 val_loss= 0.74761 val_acc= 0.79400 time= 0.14329\n",
            "Epoch: 0119 train_loss= 0.38236 train_acc= 0.98333 val_loss= 0.74699 val_acc= 0.79400 time= 0.10965\n",
            "Epoch: 0120 train_loss= 0.39862 train_acc= 0.98333 val_loss= 0.74603 val_acc= 0.79400 time= 0.10712\n",
            "Epoch: 0121 train_loss= 0.35172 train_acc= 1.00000 val_loss= 0.74456 val_acc= 0.79600 time= 0.10694\n",
            "Epoch: 0122 train_loss= 0.35832 train_acc= 0.98333 val_loss= 0.74266 val_acc= 0.79600 time= 0.10340\n",
            "Epoch: 0123 train_loss= 0.37408 train_acc= 0.95000 val_loss= 0.74078 val_acc= 0.79400 time= 0.10172\n",
            "Epoch: 0124 train_loss= 0.35291 train_acc= 0.98333 val_loss= 0.73919 val_acc= 0.79400 time= 0.10202\n",
            "Epoch: 0125 train_loss= 0.37512 train_acc= 0.98333 val_loss= 0.73780 val_acc= 0.79400 time= 0.10441\n",
            "Epoch: 0126 train_loss= 0.33184 train_acc= 1.00000 val_loss= 0.73665 val_acc= 0.79000 time= 0.10991\n",
            "Epoch: 0127 train_loss= 0.37630 train_acc= 0.98333 val_loss= 0.73555 val_acc= 0.79000 time= 0.11732\n",
            "Epoch: 0128 train_loss= 0.38041 train_acc= 0.95000 val_loss= 0.73448 val_acc= 0.78600 time= 0.10182\n",
            "Epoch: 0129 train_loss= 0.36835 train_acc= 1.00000 val_loss= 0.73350 val_acc= 0.79000 time= 0.10122\n",
            "Epoch: 0130 train_loss= 0.36854 train_acc= 1.00000 val_loss= 0.73266 val_acc= 0.79000 time= 0.13120\n",
            "Epoch: 0131 train_loss= 0.39166 train_acc= 0.93333 val_loss= 0.73201 val_acc= 0.79000 time= 0.10370\n",
            "Epoch: 0132 train_loss= 0.36407 train_acc= 0.96667 val_loss= 0.73131 val_acc= 0.79000 time= 0.09762\n",
            "Epoch: 0133 train_loss= 0.37509 train_acc= 0.98333 val_loss= 0.73047 val_acc= 0.79000 time= 0.10623\n",
            "Epoch: 0134 train_loss= 0.36168 train_acc= 0.95000 val_loss= 0.73014 val_acc= 0.79000 time= 0.10020\n",
            "Epoch: 0135 train_loss= 0.37443 train_acc= 0.98333 val_loss= 0.72971 val_acc= 0.79600 time= 0.09921\n",
            "Epoch: 0136 train_loss= 0.39505 train_acc= 0.95000 val_loss= 0.72942 val_acc= 0.79600 time= 0.10215\n",
            "Epoch: 0137 train_loss= 0.37449 train_acc= 0.95000 val_loss= 0.72934 val_acc= 0.79600 time= 0.10983\n",
            "Epoch: 0138 train_loss= 0.36171 train_acc= 1.00000 val_loss= 0.72913 val_acc= 0.79600 time= 0.10476\n",
            "Epoch: 0139 train_loss= 0.34714 train_acc= 0.98333 val_loss= 0.72868 val_acc= 0.79400 time= 0.11420\n",
            "Epoch: 0140 train_loss= 0.32881 train_acc= 1.00000 val_loss= 0.72797 val_acc= 0.79400 time= 0.11204\n",
            "Epoch: 0141 train_loss= 0.34807 train_acc= 1.00000 val_loss= 0.72669 val_acc= 0.79400 time= 0.11907\n",
            "Epoch: 0142 train_loss= 0.34317 train_acc= 0.98333 val_loss= 0.72496 val_acc= 0.79400 time= 0.10248\n",
            "Epoch: 0143 train_loss= 0.34230 train_acc= 1.00000 val_loss= 0.72286 val_acc= 0.79800 time= 0.09803\n",
            "Epoch: 0144 train_loss= 0.33331 train_acc= 1.00000 val_loss= 0.72106 val_acc= 0.79800 time= 0.09791\n",
            "Epoch: 0145 train_loss= 0.33008 train_acc= 0.96667 val_loss= 0.71976 val_acc= 0.79800 time= 0.11708\n",
            "Epoch: 0146 train_loss= 0.33520 train_acc= 1.00000 val_loss= 0.71872 val_acc= 0.80000 time= 0.10152\n",
            "Epoch: 0147 train_loss= 0.32667 train_acc= 0.98333 val_loss= 0.71813 val_acc= 0.80000 time= 0.10182\n",
            "Epoch: 0148 train_loss= 0.34408 train_acc= 1.00000 val_loss= 0.71760 val_acc= 0.80200 time= 0.11077\n",
            "Epoch: 0149 train_loss= 0.32102 train_acc= 1.00000 val_loss= 0.71719 val_acc= 0.79400 time= 0.10660\n",
            "Epoch: 0150 train_loss= 0.33770 train_acc= 0.98333 val_loss= 0.71683 val_acc= 0.79200 time= 0.10400\n",
            "Epoch: 0151 train_loss= 0.38177 train_acc= 0.98333 val_loss= 0.71667 val_acc= 0.79200 time= 0.10451\n",
            "Epoch: 0152 train_loss= 0.33061 train_acc= 1.00000 val_loss= 0.71676 val_acc= 0.79400 time= 0.10417\n",
            "Epoch: 0153 train_loss= 0.32373 train_acc= 1.00000 val_loss= 0.71708 val_acc= 0.79000 time= 0.09919\n",
            "Epoch: 0154 train_loss= 0.34795 train_acc= 0.98333 val_loss= 0.71739 val_acc= 0.79200 time= 0.10250\n",
            "Epoch: 0155 train_loss= 0.30874 train_acc= 1.00000 val_loss= 0.71766 val_acc= 0.79400 time= 0.09939\n",
            "Early stopping...\n",
            "Optimization Finished!\n",
            "Test set results: cost= 0.72828 accuracy= 0.79400 time= 0.05765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0238cnHevN7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "x, y = [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:\n",
        "    data1 = f.read().strip().split(\"\\n\")\n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_x = pca.fit_transform(x)\n",
        "\n",
        "color = ['blue', 'red']\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def update(frame):\n",
        "    ax.cla()\n",
        "    for index, item in enumerate(reduced_x):\n",
        "        if index < len(y):\n",
        "            label_index = min(y[index], len(color) - 1)\n",
        "            ax.scatter(item[0], item[1], c=color[label_index])\n",
        "    ax.set_title(\"GCN Visualization\")\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=100, interval=200)\n",
        "HTML(ani.to_jshtml())\n",
        "plt.show()\n",
        "\n",
        "# # 保存動畫\n",
        "# ani.save(\"gcn_animation.mp4\", writer=\"ffmpeg\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8DiVYdLh8rZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkS0HSU1SVl",
        "outputId": "0dc971f8-7abe-4871-e8ee-e4aa8a0f90a6"
      },
      "source": [
        "import matplotlib.pyplot as plt                 #加载matplotlib用于数据的可视化\n",
        "from sklearn.decomposition import PCA           #加载PCA算法包\n",
        "\n",
        "x, y= [], []\n",
        "with open(\"./gcn_colab/gcn/labels.txt\", \"r\") as f:  # 打开文件\n",
        "    data1 = f.read().strip().split(\"\\n\")  # 读取文件\n",
        "    for i in data1:\n",
        "        y.append(int(i))\n",
        "with open(\"./gcn_colab/gcn/embeddings.txt\", \"r\") as f:  # 打开文件\n",
        "    data1 = f.read().strip().split(\"\\n\")  # 读取文件\n",
        "    for item in data1:\n",
        "        a = []\n",
        "        item1 = item.split(\" \")\n",
        "        for i in item1:\n",
        "            a.append(float(i))\n",
        "        x.append(a)\n",
        "\n",
        "pca=PCA(n_components=2)     #加载PCA算法，设置降维后主成分数目为2\n",
        "reduced_x=pca.fit_transform(x)#对样本进行降维\n",
        "print(reduced_x)\n",
        "\n",
        "# #可视化\n",
        "color = ['blue', 'red']\n",
        "for index, item in enumerate(reduced_x):\n",
        "    label_index = min(y[index], len(color) - 1)\n",
        "    plt.scatter(item[0], item[1], c=color[label_index])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.23694408  2.30129365]\n",
            " [ 0.16284254  1.35878366]\n",
            " [-2.00710953 -1.71590138]\n",
            " ...\n",
            " [ 0.52557286  0.21814753]\n",
            " [-1.47912348 -1.81866282]\n",
            " [ 1.07136497  0.27522435]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEm1k2JOwPPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuqWccnq3XO",
        "outputId": "6e5da97c-ce25-4cf2-8e51-46a7019bfa24"
      },
      "source": [
        "\n",
        "\n",
        "# 修改train.py\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from utils import *\n",
        "from models import GCN, MLP\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask,labels = load_data(FLAGS.dataset)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "if FLAGS.model == 'gcn':\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)]  # Not used\n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.outputs], feed_dict=feed_dict)\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "# label_dict = {0:\"0\",1:\"1\",2:\"2\",3:\"3\",4:\"4\",5:\"5\",6:\"6\"} # 定义标签颜色字典\n",
        "# # 写文件\n",
        "# with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "#     for i in range(len(outs[3])):\n",
        "#         fl.write(label_dict[int(list(labels[i]).index(1.))]+\"\\n\")\n",
        "#         fe.write(\" \".join(map(str, outs[3][i]))+\"\\n\")\n",
        "label_dict = {0: \"0\", 1: \"1\"}\n",
        "# 写文件\n",
        "with open(\"./embeddings.txt\", \"w\") as fe, open(\"./labels.txt\", 'w') as fl:\n",
        "    for i in range(len(outs[3])):\n",
        "        label_index = np.argmax(labels[i])\n",
        "        fl.write(label_dict.get(label_index, str(label_index)) + \"\\n\")\n",
        "        fe.write(\" \".join(map(str, outs[3][i])) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gcn_colab'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 60 (delta 11), reused 57 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), 5.25 MiB | 1.73 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 修改utils.py\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import eigsh\n",
        "import sys\n",
        "\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def load_data(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input data from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
        "        object;\n",
        "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask,labels\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)\n"
      ],
      "metadata": {
        "id": "B9of7fy4wpVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}